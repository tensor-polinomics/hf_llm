{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799401a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '{'score': 0.9402405042710598, 'start': 78, 'end': 106, 'answer': 'Jax, PyTorch, and TensorFlow'}'\n"
     ]
    }
   ],
   "source": [
    "# QA pipeline\n",
    "from transformers import pipeline\n",
    "question_answer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "result = question_answer(question=question, context=context)\n",
    "print(f\"Answer: '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da3e93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9396279790053086,\n",
       " 'start': 1891,\n",
       " 'end': 1918,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read qa_context.txt file\n",
    "with open(\"../data/qa_context.txt\", \"r\") as f:\n",
    "    long_context = f.read()\n",
    "question_answer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc44777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ade2812871a403bb4aecbcfa0aa962c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bec7fed1db42c4903b674efa76ae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3364d3cb4e5f4d48acf4249fbf28efea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25b6ad1e63a4feb9e760e038013053b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6cce8b3a574e0bae73df94d2061d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the model from scratch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased-distilled-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 62]) torch.Size([1, 62])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out everything except the context tokens\n",
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "# Mast everything except the tokens of the context\n",
    "mask = [i !=1 for i in sequence_ids]\n",
    "\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None] # convert from list to tensor and add batch dimension at the beginning\n",
    "\n",
    "start_logits[mask] = -float(\"inf\")\n",
    "end_logits[mask] = -float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2608a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9.8204e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5015e-06, 5.3210e-06,\n",
       "         1.4555e-07, 2.4348e-06, 1.7173e-07, 2.5318e-05, 3.8216e-05, 1.3030e-06,\n",
       "         4.5736e-07, 9.0649e-06, 1.6540e-07, 8.7706e-07, 2.4123e-05, 9.9960e-01,\n",
       "         1.5769e-06, 3.1931e-05, 1.0236e-06, 2.5909e-07, 9.3109e-07, 2.2342e-07,\n",
       "         1.1279e-06, 2.2715e-04, 1.5993e-05, 6.7228e-07, 4.5640e-07, 1.1396e-07,\n",
       "         2.0751e-07, 4.3530e-08, 1.4768e-07, 4.5778e-08, 2.2926e-07, 1.0551e-07,\n",
       "         8.8707e-08, 9.1643e-09, 9.8670e-09, 7.4200e-08, 1.6864e-08, 4.2613e-08,\n",
       "         1.5442e-08, 4.4833e-08, 9.5902e-09, 5.7620e-08, 1.7640e-08, 4.7698e-08,\n",
       "         3.6061e-08, 8.7709e-09, 4.0672e-08, 3.3844e-09, 3.4904e-09, 9.6039e-09,\n",
       "         1.0284e-08, 0.0000e+00], grad_fn=<SelectBackward0>),\n",
       " tensor([1.2290e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3668e-06, 4.9776e-06,\n",
       "         7.1555e-08, 2.8872e-07, 5.0701e-08, 4.6350e-08, 6.0584e-06, 9.4446e-08,\n",
       "         9.3489e-07, 4.6811e-07, 2.6320e-06, 1.1046e-05, 4.0879e-06, 5.3092e-04,\n",
       "         3.0517e-05, 8.1017e-06, 1.2664e-05, 9.3691e-06, 5.0555e-04, 4.8219e-05,\n",
       "         1.0022e-05, 1.6710e-05, 9.9485e-01, 2.9115e-03, 6.9040e-08, 3.0433e-08,\n",
       "         6.4841e-08, 2.1654e-07, 1.1034e-06, 3.4121e-07, 9.8763e-04, 2.9021e-05,\n",
       "         1.3330e-08, 2.1349e-08, 2.6373e-08, 5.6804e-08, 1.2068e-08, 2.7114e-08,\n",
       "         2.5609e-08, 4.2720e-07, 1.3698e-08, 4.7365e-07, 2.7497e-08, 6.6031e-08,\n",
       "         1.8907e-07, 1.5993e-08, 2.2637e-07, 1.5051e-08, 1.7643e-08, 2.9421e-07,\n",
       "         7.1854e-07, 0.0000e+00], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff842ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "scores = torch.triu(scores)  # only consider end positions after start positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e31e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9945, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a487eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Jax, PyTorch, and TensorFlow'\n"
     ]
    }
   ],
   "source": [
    "# Convert token indices to character indices\n",
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "print(f\"Answer: '{answer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b47d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': 0.9944560527801514}\n"
     ]
    }
   ],
   "source": [
    "# Format result\n",
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index].item()\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a2abd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] which deep learning libraries back [UNK] transformers? [SEP] [UNK] transformers : state of the art nlp [UNK] transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. its aim is to make cutting - edge nlp easier to use for everyone. [UNK] transformers provides apis to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. at the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. why should i use transformers? 1. easy - to - use state - of - the - art models : - high performance on nlu and nlg tasks. - low barrier to entry for educators and practitioners. - few user - facing abstractions with just three classes to learn. - a unified api for using all our pretrained models. - lower compute costs, smaller carbon footprint : 2. researchers can share trained models instead of always retraining. - practitioners can reduce compute time and production costs. - dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. choose the right framework for every part of a model ' s lifetime : - train state - of - the - art models in 3 lines of code. - move a single model between tf2. 0 / pytorch frameworks at will. - seamlessly pick the right framework for training, evaluation and production. 4. easily customize a model or an example to your needs : - we provide examples for each architecture to reproduce the results published by its original authors. - model internals are exposed as consistently as possible. - model files can be used independently of the [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Handling long contexts with a sliding window\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6b2917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f61273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5959ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply masking to ignore non-context tokens\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "mask = [i !=1 for i in sequence_ids]\n",
    "mask[0] = False\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "start_logits[mask] = -float(\"inf\")\n",
    "end_logits[mask] = -float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54e413fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a72d99a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 18, 0.323265016078949), (156, 164, 0.9911271333694458)]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores). argmax().item()\n",
    "    \n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40edbb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'State of the Art NLP' (score: 0.323265016078949)\n",
      "Answer: 'Jax, PyTorch and TensorFlow' (score: 0.9911271333694458)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best answer from all the candidates\n",
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_idx, end_idx, score = candidate\n",
    "    start_char, _ = offset[start_idx]\n",
    "    _, end_char = offset[end_idx]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    print(f\"Answer: '{answer}' (score: {score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cbcee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer normalization\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a02887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "# Note: syntax changed in tokenizers v0.13.3\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a5acb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (20, 23)),\n",
       " ('?', (23, 24))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-tokenization\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are      you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83a56748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ä how', (6, 10)),\n",
       " ('Ä are', (10, 14)),\n",
       " ('Ä Ä Ä Ä Ä ', (14, 19)),\n",
       " ('Ä you', (19, 23)),\n",
       " ('?', (23, 24))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer_gpt2.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are      you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "562ce83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3199729ca9b45a4832ad407b64cdec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febbd32b709b4ca8933fa50c6b32e0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81d3b0baa3448d5bf107bb3fa71a8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('â–Hello,', (0, 6)),\n",
       " ('â–how', (7, 10)),\n",
       " ('â–are', (11, 14)),\n",
       " ('â–you?', (20, 24))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer_t5.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are      you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f7c68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b2bc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1, 'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1, 'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1, 'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_ofsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_ofsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] +=1\n",
    "        \n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36f22a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for char in word:\n",
    "        if char not in alphabet:\n",
    "            alphabet.append(char)\n",
    "alphabet.sort()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3d9b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65214926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', 'h', 'i', 's'], 'Ä is': ['Ä ', 'i', 's'], 'Ä the': ['Ä ', 't', 'h', 'e'], 'Ä Hugging': ['Ä ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'], 'Ä Face': ['Ä ', 'F', 'a', 'c', 'e'], 'Ä Course': ['Ä ', 'C', 'o', 'u', 'r', 's', 'e'], '.': ['.'], 'Ä chapter': ['Ä ', 'c', 'h', 'a', 'p', 't', 'e', 'r'], 'Ä about': ['Ä ', 'a', 'b', 'o', 'u', 't'], 'Ä tokenization': ['Ä ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Ä section': ['Ä ', 's', 'e', 'c', 't', 'i', 'o', 'n'], 'Ä shows': ['Ä ', 's', 'h', 'o', 'w', 's'], 'Ä several': ['Ä ', 's', 'e', 'v', 'e', 'r', 'a', 'l'], 'Ä tokenizer': ['Ä ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'Ä algorithms': ['Ä ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'], 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'], ',': [','], 'Ä you': ['Ä ', 'y', 'o', 'u'], 'Ä will': ['Ä ', 'w', 'i', 'l', 'l'], 'Ä be': ['Ä ', 'b', 'e'], 'Ä able': ['Ä ', 'a', 'b', 'l', 'e'], 'Ä to': ['Ä ', 't', 'o'], 'Ä understand': ['Ä ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'Ä how': ['Ä ', 'h', 'o', 'w'], 'Ä they': ['Ä ', 't', 'h', 'e', 'y'], 'Ä are': ['Ä ', 'a', 'r', 'e'], 'Ä trained': ['Ä ', 't', 'r', 'a', 'i', 'n', 'e', 'd'], 'Ä and': ['Ä ', 'a', 'n', 'd'], 'Ä generate': ['Ä ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'Ä tokens': ['Ä ', 't', 'o', 'k', 'e', 'n', 's']}\n"
     ]
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7be53a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute word pair frequencies\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i], split[i+1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77824885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ä ', 'i'): 2\n",
      "('Ä ', 't'): 7\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"{key}: {pair_freqs[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dcf81ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pair: ('Ä ', 't') with frequency 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or freq > max_freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "        \n",
    "print(f\"Best pair: {best_pair} with frequency {max_freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe07096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'Ä t']\n"
     ]
    }
   ],
   "source": [
    "merges = {(\"Ä \", \"t\"): \"Ä t\"}\n",
    "vocab.append(\"Ä t\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54722694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to merge tokens based on the merges dictionary\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) -1:\n",
    "            if split[i] == a and split[i+1] == b:\n",
    "                split = split[:i] + [merges[(a, b)]] + split[i+2:]\n",
    "            else:\n",
    "                i +=1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "595326a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"Ä \", \"t\", splits)\n",
    "print(splits[\"Ä trained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "231c9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or freq > max_freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    \n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "    splits = merge_pair(*best_pair, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32d42e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok', ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the', ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fa443ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se', 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2c992f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2250b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfeef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
