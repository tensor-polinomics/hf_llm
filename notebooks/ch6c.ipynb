{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ded8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3785ed43df4409ba03623b39c20ab2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4d2302833541c1b109d73983c344dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe759b1c14b49489eb5a68b79120eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbb0be918244353bf5c29869d354e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d45bc5487546518ab67f0a217e4c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d8db2fbe5f49b08e188f903155a02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba90c57e2f742b1bf075578bfc54730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train your own tokenizer\n",
    "# Acquiring a corpus\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Build a generator\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40823af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a text file\n",
    "with open(\"../data/wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814a3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a WordPiece tokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "# Specify unknown token\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd87d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set normalizer, using BertNormalizer\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154b4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BertNormalizer by hand\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98fe3264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "# Check out an example\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e0b1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenize using prebuilt BertPreTokenizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf53f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build one from scratch\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216af758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65b08b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.Whitespace(), pre_tokenizers.Punctuation()])\n",
    "\n",
    "# Assign to tokenizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f89197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a trainer\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8424da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be9726a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test it out\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b81f261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# Get the IDs of special tokens\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d9641c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set BERT-style template\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", cls_token_id),\n",
    "        (\"[SEP]\", sep_token_id),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58a031c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59100bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '.', '.', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff1e855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a decoder\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "096838d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee302d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer to disk\n",
    "tokenizer.save(\"../data/wordpiece-tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f18851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load our tokenizer\n",
    "new_tokenizer = Tokenizer.from_file(\"../data/wordpiece-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4301586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our tokenizer in a PreTrainedTokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=new_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['L', 'et', \"'\", 's', 'Ġtest', 'Ġmy', 'ĠBP', 'E', 'Ġto', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Build a BPE tokenizer from scratch\n",
    "bpe_tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# No need for normalizer\n",
    "\n",
    "# Add pre-tokenizer (no space prefix)\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "bpe_tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my BPE tokenizer.\")\n",
    "\n",
    "# Build a trainer\n",
    "bpe_trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "bpe_tokenizer.train_from_iterator(get_training_corpus(), trainer=bpe_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1175fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'et', \"'\", 's', 'Ġtest', 'Ġmy', 'ĠBP', 'E', 'Ġto', 'ken', 'izer', '.']\n",
      "[(0, 1), (1, 3), (3, 4), (4, 5), (5, 10), (10, 13), (13, 16), (16, 17), (17, 20), (20, 23), (23, 27), (27, 28)]\n",
      " test\n"
     ]
    }
   ],
   "source": [
    "# Add post-processor\n",
    "bpe_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "sentence = \"Let's test my BPE tokenizer.\"\n",
    "bpe_encoding = bpe_tokenizer.encode(sentence)\n",
    "print(bpe_encoding.tokens)\n",
    "print(bpe_encoding.offsets)\n",
    "start, end = bpe_encoding.offsets[4]\n",
    "print(sentence[start:end])\n",
    "\n",
    "# Add a byte-level decoder\n",
    "bpe_tokenizer.decoder = decoders.ByteLevel()\n",
    "bpe_tokenizer.decode(bpe_encoding.ids)\n",
    "\n",
    "# Wrap in PreTrainedTokenizerFast\n",
    "bpe_wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=bpe_tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d397b02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a Unigram tokenizer from scratch\n",
    "unigram_tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "# Add pre-tokenizer\n",
    "from tokenizers import Regex\n",
    "unigram_tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add pre-tokenizer\n",
    "unigram_tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "unigram_tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my Unigram tokenizer.\")\n",
    "\n",
    "# Build a trainer\n",
    "special_tokens = [\"<cls>\", \"<sep>\", \"<pad>\", \"<unk>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "unigram_trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, \n",
    "    special_tokens=special_tokens,\n",
    "    unk_token=\"<unk>\",\n",
    ")\n",
    "unigram_tokenizer.train_from_iterator(get_training_corpus(), trainer=unigram_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3619a98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁my', '▁Un', 'i', 'gram', '▁to', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "unigram_encoding = unigram_tokenizer.encode(\"Let's test my Unigram tokenizer.\")\n",
    "print(unigram_encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06797227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = unigram_tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = unigram_tokenizer.token_to_id(\"<sep>\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e75018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"<cls>:0 $A:0 <cls>:2\",\n",
    "    pair=f\"$A:0 <sep>:0 $B:1 <cls>:2\",\n",
    "    special_tokens=[\n",
    "        (\"<cls>\", cls_token_id),\n",
    "        (\"<sep>\", sep_token_id),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28a366c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "unigram_encoding = unigram_tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
    "print(unigram_encoding.tokens)\n",
    "print(unigram_encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aad3e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a MetaSpace decoder\n",
    "unigram_tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03d6b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in PreTrainedTokenizerFast\n",
    "unigram_wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=unigram_tokenizer,\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f8611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
