{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1dc50b5",
   "metadata": {},
   "source": [
    "## Fine-tuning a masked language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f05727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae1b05499c344c9961395dc8ec04ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d13855097944cc7ad8f938a94f0da1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb9f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT has 66.99 million parameters.\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"DistilBERT has {distilbert_num_parameters:.2f} million parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b41af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Well, another large [MASK] model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e458b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197a84788deb405d880b2f89c073b8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49db59ececb4aa59a8670981ae9c166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfac4cbdcdfa4421b350173bc3924496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5de714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 30522])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find the top 5 predictions for the masked token\n",
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs_logits = model(**inputs).logits\n",
    "inputs_logits.shape # batch size, sequence length, vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b312b819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'scale'\n",
      ">>> Well, another large scale model.\n",
      "'satellite'\n",
      ">>> Well, another large satellite model.\n",
      "'business'\n",
      ">>> Well, another large business model.\n",
      "'size'\n",
      ">>> Well, another large size model.\n",
      "'sized'\n",
      ">>> Well, another large sized model.\n"
     ]
    }
   ],
   "source": [
    "# Find the location of the masked token\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = inputs_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'{tokenizer.decode([token])}'\")\n",
    "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf47e460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d73ae2cbf674684b4d67639dab04e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d8f2622a8e4fa89724a9d77726c922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8909659a1349ea8b8ac22d31e3fc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7594ce473b32415b8b3784d0fcff61f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b89686e389b48b68592858c6e91f22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9ba5778f09407db5922a10225aafe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93904f4d1624dabba0a716dd38901c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load imdb movie reviews dataset\n",
    "from datasets import load_dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f403457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      "Label: 1\n",
      "Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      "Label: 1\n",
      "Review: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "samples = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for sample in samples:\n",
    "    print(f\"Review: {sample['text']}\")\n",
    "    print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a53d29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: If you've seen the classic Roger Corman version starring Vincent Price it's hard to put it out of your head, but you probably should do because this one is totally different. Subtlety has been abandoned in favour of gross-out horror - nudity, gore and all-round unpleasantness. OK it's ridiculous, trashy, sensationalised and historically dubious (did any members of the Inquisition really wear horn-rimmed glasses?), but despite all this it is strangely compelling. I literally couldn't tear myself away from the screen until the end of the movie. If there's a bigger compliment you can pay to a film I don't know what it is.\n",
      "Label: -1\n",
      "Review: For me, this was the most moving film of the decade. Samira Makhmalbaf shows pure bravery and vision in the making. She has an intelligence and gift for speaking to the people, regardless of their nationality or beliefs. I am inspired and touched by her humanity and can only hope that she has touched many people the same way. Her message in this film is strong, simple and pure. The human soul can survive the most unheard of cruelties and repression, yet still have the capability to hope and dream even the biggest dreams. Under the most incredible circumstances, the most unexpected people rise up to be heroes. This young girl who has recently regained her voice, yet is still afraid to use her new found freedom, is our hero. She daydreams of becoming president of war torn Afghanistan, the only vision of power that she can imagine that could truly change her current situation. We catch a glimpse of her spirit while witnessing her hardships. In the end, we are left with hope, hope that when her young voice does eventually speak out, it speaks loud and clear for all to hear- sounding a message that transcends borders, nationality and religion. The true epitome of the phoenix rising from the ashes. Hats off to the simple tale of the complex truth.\n",
      "Label: -1\n",
      "Review: There really isn't much to say about this \"film\". It has the odd smile or chuckle moment, but on the whole it's bland, predictable and generally pretty dull.<br /><br />The only reason I gave it three out of ten was for the annoyingly catchy jingle (which I hope I will forget soon....please God!). Otherwise its junk. Or mostly junk, interspersed with adverts for Smirnoff Ice.<br /><br />The lead characters give OK performances, but they really don't have anything much to work with.<br /><br />Best advice: Avoid it like a dentist's appointment. Or better yet, make a dentist's appointment instead of watching it.\n",
      "Label: -1\n"
     ]
    }
   ],
   "source": [
    "# Unlabled data has label -1\n",
    "samples = imdb_dataset[\"unsupervised\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for sample in samples:\n",
    "    print(f\"Review: {sample['text']}\")\n",
    "    print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c454e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 216, 217, 218, 218, 219, 220, 221, 222, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 272, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, None], [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 91, 91, 92, 93, 93, 94, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 110, 111, 112, 113, 114, 114, 114, 114, 115, 116, 117, 118, 119, 120, 121, 122, 122, 123, 124, 125, 126, 127, 128, 128, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 162, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 178, 179, 180, 181, 182, 182, 182, 183, 184, 185, 186, 187, 188, 188, 188, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 200, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 228, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 254, 255, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, None]]\n"
     ]
    }
   ],
   "source": [
    "# Define a tokenize function and test it\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [\n",
    "            result.word_ids(i) for i in range(len(result[\"input_ids\"]))\n",
    "        ]\n",
    "    return result\n",
    "\n",
    "examples = imdb_dataset[\"train\"][:2]\n",
    "tokenized_examples = tokenize_function(examples)\n",
    "print(tokenized_examples[\"word_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb0324cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6515c6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[363, 304]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(ids) for ids in tokenized_examples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17faee20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9e08cb3a704034b8fd1070149b883d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19e6aa06968494e943b8faeaa87c7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc89694eb114293b52123b4450c2137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f765b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the maximum length (context size) of the tokenizer\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c704597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our chunk size\n",
    "chunk_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1bb26f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 has 363 tokens.\n",
      "Sample 1 has 304 tokens.\n",
      "Sample 2 has 133 tokens.\n",
      "Sample 3 has 185 tokens.\n",
      "Sample 4 has 495 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Check out a few examples\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:5]\n",
    "for i, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"Sample {i} has {len(sample)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed8d01f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated length: 1480 tokens\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all samples\n",
    "# sum(..., []) flattens a list of lists\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"Concatenated length: {total_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "323f6934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk of size 512: [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066]\n",
      "Chunk of size 512: [1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102, 101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102, 101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102, 101, 2821, 1010, 2567, 1012, 1012, 1012, 2044, 4994, 2055, 2023, 9951, 2143, 2005, 8529, 13876, 12129, 2086, 2035, 1045, 2064, 2228, 1997, 2003, 2008, 2214, 14911, 3389, 2299, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028]\n",
      "Chunk of size 456: [1000, 2003, 2008, 2035, 2045, 2003, 1029, 1029, 1000, 1012, 1012, 1012, 1045, 2001, 2074, 2019, 2220, 9458, 2043, 2023, 20482, 3869, 2718, 1996, 1057, 1012, 1055, 1012, 1045, 2001, 2205, 2402, 2000, 2131, 1999, 1996, 4258, 1006, 2348, 1045, 2106, 6133, 2000, 13583, 2046, 1000, 9119, 8912, 1000, 1007, 1012, 2059, 1037, 11326, 2012, 1037, 2334, 2143, 2688, 10272, 17799, 1011, 2633, 1045, 2071, 2156, 2023, 2143, 1010, 3272, 2085, 1045, 2001, 2004, 2214, 2004, 2026, 3008, 2020, 2043, 2027, 8040, 7317, 13699, 5669, 2000, 2156, 2009, 999, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 3114, 2023, 2143, 2001, 2025, 10033, 2000, 1996, 10812, 13457, 1997, 2051, 2001, 2138, 1997, 1996, 27885, 11020, 20693, 2553, 13977, 2011, 2049, 1057, 1012, 1055, 1012, 2713, 1012, 8817, 1997, 2111, 19311, 2098, 2000, 2023, 27136, 2121, 1010, 3241, 2027, 2020, 2183, 2000, 2156, 1037, 3348, 2143, 1012, 1012, 1012, 2612, 1010, 2027, 2288, 7167, 1997, 2485, 22264, 1997, 1043, 11802, 2135, 1010, 16360, 23004, 25430, 18352, 1010, 2006, 1011, 2395, 7636, 1999, 20857, 6023, 25943, 1010, 2004, 5498, 2063, 2576, 3653, 29048, 1012, 1012, 1012, 1998, 7408, 3468, 2040, 1011, 14977, 23599, 3348, 5019, 2007, 7842, 22772, 1010, 5122, 5889, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3451, 12696, 1010, 4151, 24665, 12502, 1010, 3181, 20785, 1012, 1012, 3649, 2023, 2518, 2001, 1010, 14021, 5596, 2009, 1010, 6402, 2009, 1010, 2059, 4933, 1996, 11289, 1999, 1037, 2599, 3482, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7069, 9765, 27065, 2229, 2145, 26988, 2000, 2424, 3643, 1999, 2049, 11771, 18404, 6208, 2576, 11867, 7974, 8613, 1012, 1012, 2021, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 15657, 9446, 1010, 2009, 2052, 2031, 2042, 6439, 1010, 2059, 6404, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2612, 1010, 1996, 1000, 1045, 2572, 8744, 1010, 8744, 1000, 1054, 10536, 16921, 7583, 2516, 2001, 5567, 10866, 2135, 2005, 2086, 2004, 1037, 14841, 26065, 3508, 2005, 22555, 2080, 3152, 1006, 1045, 2572, 8025, 1010, 20920, 1011, 2005, 5637, 3152, 1010, 1045, 2572, 8025, 1010, 2304, 1011, 2005, 1038, 2721, 2595, 24759, 28100, 3370, 3152, 1010, 4385, 1012, 1012, 1007, 1998, 2296, 2702, 2086, 2030, 2061, 1996, 2518, 9466, 2013, 1996, 2757, 1010, 2000, 2022, 7021, 2011, 1037, 2047, 4245, 1997, 26476, 2015, 2040, 2215, 2000, 2156, 2008, 1000, 20355, 3348, 2143, 1000, 2008, 1000, 4329, 3550, 1996, 2143, 3068, 1000, 1012, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6300, 9953, 1010, 4468, 2066, 1996, 11629, 1012, 1012, 2030, 2065, 2017, 2442, 2156, 2009, 1011, 9278, 1996, 2678, 1998, 3435, 2830, 2000, 1996, 1000, 6530, 1000, 3033, 1010, 2074, 2000, 2131, 2009, 2058, 2007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i: i+chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"Chunk of size {len(chunk)}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97729a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to group texts into chunks\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {\n",
    "        k: sum(examples[k], []) for k in examples.keys()\n",
    "    }\n",
    "    total_length = len(concatenated_examples[\"input_ids\"])\n",
    "    # We drop the small remainder\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Add a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbfe05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719210c7ae1141368c5de79434b95030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234612c950944d3a896108432023debf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ce99589ddb4765bbae9e596400784d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the language modeling dataset\n",
    "# This will take about 2 minutes\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts, batched=True, batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45001231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 15313\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 14966\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 30721\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70251606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. s. customs if it ever tried to enter this country, therefore being a fan of films considered \" controversial \" i really had to see this for myself. < br / > < br / > the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it \\' s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman, arguably their answer to good old boy john ford, had sex scenes in his films. < br / > < br / > i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for anyone wanting to study the meat and potatoes ( no pun intended ) of swedish cinema. but really, this film doesn \\' t have much of a plot. [SEP] [CLS] \" i am curious : yellow \" is a risible and pretentious steaming pile. it doesn \\' t matter what one \\' s political views are because this film can hardly be taken seriously on any level. as for the claim that frontal male nudity is an automatic nc - 17, that isn \\' t true. i \\' ve seen r - rated films with male nudity. granted, they only offer some fleeting views, but where are the r - rated films with gaping vulvas and flapping labia? nowhere, because they don \\' t exist. the same goes for those crappy cable shows : schlongs swinging in the breeze but not a clitoris in sight. and those pretentious indie movies like'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f736a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a data collator for language modeling\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb439716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: [CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first [MASK] was seized by [MASK]. [MASK]. customs if it ever tried to [MASK] this country, therefore [MASK] a fan of [MASK] [MASK] \" controversial \" i really had to see this for myself. < br / > [MASK] br [MASK] > the plot is centered [MASK] a young [MASK] drama [MASK] named lena who wants to learn everything she can about life. in particular she wants [MASK] [MASK] her attentions to [MASK] some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in [MASK] united states. in between [MASK] politicians and [MASK] denizens information stockholm about their opinions on politics, she has sex with her drama teacher, classmates, [MASK] married men. < br / > < br / > what kills mecape i am curious - yellow is that 40 years [MASK], this was considered pornographic. really, the sex and nudity [unused666] are few and far between [MASK] even then it ' s not shot like some cheaply made porno [MASK] [MASK] my countrymen mind find [MASK] shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman, arguably their answer to good old boy [MASK] [MASK], had sex scenes in his films. [MASK] br / > < br [MASK] > i do commen [MASK] the filmmakers for the fact that any sex shown in meanings film variability shown for [MASK] purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i [MASK] curious - yellow is [MASK] good film for anyone wanting to study the meat and potatoes ( no pun intended ) of swedish cinema. but really, this film doesn ' t [MASK] much of a plot [MASK] [SEP] [CLS] \" i am curious : yellow \" is a ri [MASK] and pretentious steaming pile. it doesn ' t [MASK] what one [MASK] s [MASK] views are because this film can hardly be taken seriously on any level. as for the claim that frontal male nu [MASK] is [MASK] automatic nc - 17, that isn ' [MASK] true. [MASK] ' ve seen r [MASK] rated films with male nudity. granted, they only offer some fleeting views, but where are the r [MASK] rated films with gaping [MASK]lva [MASK] and flapping lab [MASK]? nowhere, because they don [MASK] t exist. the same [MASK] for those crappy cable shows : schlongs swinging in the breeze but not a clit marshall in sight. [MASK] those preten [MASK] indie movies [MASK]\n",
      "Chunk: lever brown bunny, [MASK] which we ' re treated to [MASK] site of [MASK] gallo ' s throbbing johnson, but not a trace of [MASK]ups on chloe sevigny. before crying ( or [MASK] ) \" double - standard \" in matters of nudity, the mentally obtus [MASK] should take into account one una [MASK] [MASK]bly obvious anatomical difference between men and women : there are no [MASK]itals on display when actresses appears nude, [MASK] the same cannot be said for [MASK] man. in fact, you [MASK] won ' t see female genitals in [MASK] [MASK] film in anything short of porn or [MASK] erotica. this alleged [MASK] - standard is less [MASK] double [MASK] than flirt [MASK]ly depressing ability to come [MASK] terms culturally with the insides of women ' s [MASK]. [SEP] [CLS] if only [MASK] avoid making this type [MASK] film in the future. this film is [MASK] as an [MASK] but tells no [MASK]gent story. < br / > < br / > [MASK] might feel virtuous for sitting [MASK] [MASK] because it touches on so many important issues [MASK] it does so without any disc [MASK]able motive. the viewer comes away with [MASK] [MASK] perspectives ( unless one [MASK] up with one while [MASK] ' s mind wanders [MASK] as it will invariably do [MASK] this pointless film ). < br / > < br / > one [MASK] better spend one ' s time staring out a window at a tree [MASK]. < br / > < br / > [SEP] [CLS] this film was probably inspired by god [MASK] ' [MASK] masculin, [MASK]minin and i urge you to see und film instead. < br / [MASK] < br / > the film has two strong elements and those are, ( 1 [MASK] the realistic [MASK] ( 2 ) lazy impressive [MASK] undeservedly good, [MASK]. apart from that, what [MASK] me blu [MASK]ƒ endless stream of silliness. lena nyman has to be most annoying actress in the world. she acts so [MASK] and with [MASK] [MASK] nudity in this film,... it ' s unattractive. comparing to godard [MASK] s film, intellectuality has been replaced [MASK] [MASK]. without going too [MASK] on this subject, i would [MASK] [MASK] follows [MASK] the difference in ideals [MASK] [MASK] french and the [MASK] society. < br / > < br / > a movie of its time [MASK] and place. 2 / 10 [MASK] [SEP] [CLS] oh, [MASK] [MASK].. after hearing about this ridiculous [MASK] for umpteen years all i can think of is that old peggy lee gnome.. < [MASK] / > < br / >\n",
      "Chunk: \" is that all there is?? [MASK]... i was just an early teen when this smoked fish hit [MASK] u. s. i was too young [MASK] get [MASK] the theater ( although i did manage to sneak [MASK] \" [MASK] columbus \" ). then [MASK] screening at [MASK] local film museum beckoned [MASK] finally [MASK] could see this [MASK] [MASK] [MASK]going [MASK] was [MASK] old as my parents [MASK] when they schlepped [MASK] see it!! < br / > < br / > [MASK] only [MASK] this [MASK] was not romano to the anonymous sands ofpia was because of the obscenity case sparked [MASK] its u. s. release. millions of people [MASK]ed to [MASK] stinker, thinking they [MASK] [MASK] to see a sex [MASK]. [MASK]. [MASK], they got [MASK] of closeups of [MASK]narly, [MASK]ulsive swedes, on - street interviews in evans shopping malls, asinie [MASK] pre [MASK].. yukon and feeble who - cares simulated sex scenes [MASK] saggy, pale actors. < br / > [MASK] br / [MASK] [MASK] icon, holy grail, historic artifact.. whatever [MASK] thing was, shred it [MASK] burn it, then stuff the ashes in a lead box! < [MASK] / > < br / > elite esthet [MASK] [MASK] scrape to find value in its boring pseudo revolutionary political spewings.. but if [MASK] weren ' t for the censorship scandal, [MASK] would have [MASK] ignored, then forgotten. [MASK] br / > conspicuous br / > instead, the \" i am blank, blank \" rhythymed title [MASK] [MASK] [MASK] [MASK] for years [MASK] [MASK] ti [MASK]tion for porno films ( i am curious, lavender - [MASK] gay [MASK] [MASK] i am curious, black - for [MASK]laxploitation films [MASK] etc. paintings ) and [MASK] ten years or so the thing rises from the dead, to be viewed by [MASK] new generation of suckers who want to see caracas \" undefeated sex film \" that \" revolutionized the film industry [MASK]... < br / pushed < br / > yeesh, avoid like the plague [MASK]. or if you must see it - rent the video and fast forward to the \" dirty \" parts, just to get it over with. < br / [MASK] < br / > [SEP] [CLS] i would put this at the top of my listpa films [MASK] the category of unwatch [MASK] trash [MASK] there are films that are bad, but the worst kind are [MASK] [MASK] that [MASK] unwatch [MASK] but you are suppose [MASK] like them because they are supposed to be [MASK] for\n"
     ]
    }
   ],
   "source": [
    "# Let's see how this masking works\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(3)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"Chunk: {tokenizer.decode(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5c9db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a custom data collator that masks only whole words\n",
    "import collections\n",
    "import numpy as np\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.20\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "        # Create a mapping of word to its token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_index in enumerate(word_ids):\n",
    "            if word_index is not None:\n",
    "                if word_index != current_word:\n",
    "                    current_word = word_index\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "                \n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_index in np.where(mask)[0]:\n",
    "            word_index = word_index.item()\n",
    "            for token_index in mapping[word_index]:\n",
    "                new_labels[token_index] = labels[token_index]\n",
    "                input_ids[token_index] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ceceb113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: [CLS] i [MASK] i [MASK] [MASK] - yellow from [MASK] video [MASK] [MASK] of [MASK] the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. s [MASK] customs if it ever [MASK] to enter this country, [MASK] being a fan of films considered \" [MASK] \" i really [MASK] to see this for myself. < br / > < br / > the plot is centered around [MASK] [MASK] [MASK] drama [MASK] named lena who wants to learn everything she [MASK] about life. in particular [MASK] wants to focus her attentions [MASK] making some sort of documentary on [MASK] [MASK] average swede thought about certain political [MASK] such as the vietnam war and [MASK] issues in [MASK] united states. in between asking politicians and ordinary denizens of stockholm [MASK] their opinions on politics, she [MASK] sex with [MASK] drama teacher, classmates, and married men. < br / > < br [MASK] > what kills me about [MASK] [MASK] curious [MASK] yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, [MASK] [MASK] it ' s not [MASK] [MASK] some cheaply made porno. [MASK] [MASK] countrymen mind [MASK] it shocking [MASK] [MASK] reality sex [MASK] [MASK] [MASK] [MASK] a major staple in swedish cinema [MASK] [MASK] ingmar [MASK] [MASK] arguably their answer to good [MASK] boy john ford, [MASK] sex scenes in his films. < br [MASK] > < br / > i [MASK] commend the [MASK] for [MASK] fact that any [MASK] shown in the film [MASK] [MASK] for artistic purposes rather [MASK] just to shock people [MASK] make money to be shown in [MASK] theaters in america [MASK] [MASK] am curious - yellow is a [MASK] film for [MASK] [MASK] to [MASK] the meat and [MASK] ( no pun intended ) of swedish cinema [MASK] [MASK] really, this [MASK] [MASK] ' t have much of a plot. [SEP] [CLS] \" i am curious : yellow [MASK] is a risible and pretentious steaming pile. it doesn ' t matter what [MASK] ' s [MASK] views are because this [MASK] can hardly be [MASK] [MASK] on [MASK] level. as for the [MASK] that [MASK] male nudity is [MASK] automatic nc - 17, that isn ' t [MASK]. i ' ve seen r - rated films with male nudity [MASK] granted [MASK] they only offer some fleeting [MASK], but where are [MASK] r - rated films [MASK] gaping vulvas and [MASK] [MASK] labia? [MASK], because [MASK] don ' t exist. [MASK] same goes for those crappy cable shows : schlongs swinging in the breeze but not a clitoris in sight. [MASK] [MASK] pretentious indie [MASK] like\n",
      "Chunk: the brown bunny, in which [MASK] ' re [MASK] to [MASK] site of [MASK] gallo ' s [MASK] johnson, but [MASK] [MASK] trace [MASK] pink visible on chloe sevigny. before crying ( or implying ) \" double - standard \" in matters of nudity, the mentally obtuse should take [MASK] account one [MASK] [MASK] [MASK] [MASK] [MASK] anatomical difference between men [MASK] women : there are no genitals [MASK] display when actresses appears nude, and the same cannot [MASK] said for a man. in fact, you generally won ' t see female genitals in an american film [MASK] anything short [MASK] [MASK] or [MASK] erotica. this [MASK] double - [MASK] is [MASK] a double standard than an admittedly depressing [MASK] to come [MASK] terms culturally with the insides [MASK] women [MASK] [MASK] bodies. [SEP] [CLS] if only to avoid making this type [MASK] film in the future. [MASK] film [MASK] interesting [MASK] an experiment but [MASK] [MASK] cogent story. < br / > [MASK] br / > one [MASK] feel virtuous for sitting thru it because it touches on [MASK] many important issues but [MASK] does so without [MASK] discernable motive. the viewer comes away with no [MASK] perspectives ( unless [MASK] [MASK] up with one while one [MASK] s mind wanders, as it will [MASK] [MASK] during this pointless film [MASK] [MASK] < br / > < br / > [MASK] might better spend one ' s time staring out a window [MASK] a tree [MASK]. < br [MASK] > < br [MASK] > [SEP] [CLS] [MASK] film was probably inspired by godard ' s masculin, feminin and i urge you to [MASK] that film instead [MASK] < [MASK] / > [MASK] br / [MASK] the film has [MASK] strong elements and those [MASK], ( 1 ) the realistic acting ( 2 ) the impressive, undeservedly [MASK], photo. apart from that, what strikes me [MASK] is the [MASK] stream of [MASK] [MASK] [MASK]. lena [MASK] [MASK] has to be most [MASK] actress in the world. she [MASK] so stupid and with all the [MASK] [MASK] [MASK] [MASK] film,.. [MASK] it ' [MASK] unattractive. comparing to godard ' s film, [MASK] [MASK] [MASK] been replaced with stupidity. [MASK] [MASK] too far on this [MASK], i would say that follows from the difference in ideals between the french and the swedish society. < br / > [MASK] [MASK] / > a movie of its [MASK], [MASK] place. 2 / 10. [SEP] [CLS] oh, brother... after hearing about this ridiculous film for [MASK] [MASK] [MASK] years all i can think of is that old peggy lee song.. < br / > < br [MASK] >\n",
      "Chunk: \" is that all there is?? \". [MASK]. i was just an early teen when this smoked fish [MASK] [MASK] u. s. i was [MASK] young to get in [MASK] theater ( although i did manage to sneak into \" [MASK] columbus \" ). then [MASK] screening at a local [MASK] museum beckoned - [MASK] i could see this film [MASK] except now i was as old as my parents [MASK] [MASK] they schlepped to see it!! [MASK] [MASK] / > < br / [MASK] the only reason this film was [MASK] condemned to [MASK] anonymous sands [MASK] [MASK] was because of the obscenity [MASK] sparked [MASK] its u. s [MASK] release. millions of people [MASK] [MASK] [MASK] this stinker [MASK] thinking they were [MASK] [MASK] see a sex [MASK]. [MASK]. instead, they got lots of closeups of [MASK] [MASK] [MASK], repulsive swedes, on - street interviews in bland shopping malls [MASK] asinie [MASK] pretension. [MASK]. and [MASK] [MASK] [MASK] [MASK] cares simulated sex scenes with [MASK] [MASK], pale actors [MASK] < br / [MASK] < br / > [MASK] icon, holy grail, historic [MASK]. [MASK] whatever this thing was, shred it, burn it, then stuff the [MASK] in a lead box [MASK] [MASK] br / > [MASK] [MASK] / [MASK] elite esthetes [MASK] scrape to find [MASK] [MASK] [MASK] boring pseudo revolutionary political spewings [MASK]. but [MASK] [MASK] weren [MASK] t [MASK] the [MASK] [MASK] [MASK] it would [MASK] [MASK] ignored, [MASK] [MASK]. [MASK] br / > < br / > instead [MASK] the [MASK] i am [MASK] [MASK] blank [MASK] rhythymed [MASK] was repeated endlessly for [MASK] as a [MASK] [MASK] [MASK] for [MASK] [MASK] films ( i am [MASK], lavender - [MASK] gay films, [MASK] am curious, [MASK] - for blaxploitation films [MASK] etc.. ) and every ten years or [MASK] [MASK] thing rises from the dead, to be viewed by a [MASK] generation of suckers who want [MASK] see that \" naughty sex film [MASK] that \" revolutionized the film industry [MASK].. [MASK] < br / [MASK] < br / > yeesh, avoid like the plague.. [MASK] [MASK] you must see it - [MASK] the [MASK] and fast forward to [MASK] \" dirty [MASK] parts, just to [MASK] it [MASK] with [MASK] < br / > < br / > [SEP] [CLS] i would put this [MASK] the top of [MASK] list of films in the category [MASK] [MASK] [MASK] [MASK] trash [MASK] there are [MASK] that [MASK] bad, but the worst kind are [MASK] ones that are unwatchable but you [MASK] suppose to like them because they are [MASK] [MASK] [MASK] good [MASK]\n"
     ]
    }
   ],
   "source": [
    "# Let's see how this masking works\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(3)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"Chunk: {tokenizer.decode(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0e94ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Down sample our dataset for faster training\n",
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbdb8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hf credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN_WRITE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f59adf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_4334/1117272117.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Directory settings\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"../data/cache\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # if not using wandb\n",
    "\n",
    "# Build the trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../data/{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"{model_name}-finetuned-imdb\",\n",
    "    hub_token=token,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52057a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 19.15\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity (lower is better)\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "perplexity = math.exp(eval_results[\"eval_loss\"])\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a60e5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 02:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.526600</td>\n",
       "      <td>2.326373</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.424400</td>\n",
       "      <td>2.275798</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.406900</td>\n",
       "      <td>2.265360</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=471, training_loss=2.4524260205068407, metrics={'train_runtime': 166.3414, 'train_samples_per_second': 180.352, 'train_steps_per_second': 2.832, 'total_flos': 3976834682880000.0, 'train_loss': 2.4524260205068407, 'epoch': 3.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8a31743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 9.56\n"
     ]
    }
   ],
   "source": [
    "# Perplexity after training\n",
    "eval_results = trainer.evaluate()\n",
    "perplexity = math.exp(eval_results[\"eval_loss\"])\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57ae8d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c445d801be742019dd368dd13186b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd747c9d7f745da83257c4986c6f9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tensor-polinomics/distilbert-base-uncased-finetuned-imdb/commit/5a1d75969cf75ff716eb5657faaea214af7128a8', commit_message='End of training', commit_description='', oid='5a1d75969cf75ff716eb5657faaea214af7128a8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tensor-polinomics/distilbert-base-uncased-finetuned-imdb', endpoint='https://huggingface.co', repo_type='model', repo_id='tensor-polinomics/distilbert-base-uncased-finetuned-imdb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our model to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93465130",
   "metadata": {},
   "source": [
    "## Fine-tuning DistilBERT with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfa029c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce randomness in results, let's apply the masking once on the whole dataset\n",
    "# Build a helper function\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Return a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask, batched=True, batch_size=1000,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb304a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "880a1944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a463c9a7a94ff2ac4ba41ecf9d7042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Perplexity: 9.57\n",
      "Epoch 1: Perplexity: 9.10\n",
      "Epoch 2: Perplexity: 8.94\n",
      "Repo created: tensor-polinomics/distilbert-finetuned-imdb-accelerate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50403ec1f804708b4fc8d250dee3c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509e253a8a90440ea8aded4d3fd4209b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c69576b8fbe4b29b101f5a279e632c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload the model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Optimizer\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set up the Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "ProgressBar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        ProgressBar.update(1)\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    \n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    print(f\"Epoch {epoch}: Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "# Save and upload\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "\n",
    "model_name = \"distilbert-finetuned-imdb-accelerate\"\n",
    "\n",
    "# Create repo\n",
    "repo_id = create_repo(model_name, token=token, exist_ok=True).repo_id\n",
    "print(f\"Repo created: {repo_id}\")\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(\n",
    "    \"../data/models/distilbert-finetuned-imdb-accelerate\",\n",
    "    save_function=accelerator.save,\n",
    ")\n",
    "if accelerator.is_main_process:\n",
    "    tokenizer.save_pretrained(\"../data/models/distilbert-finetuned-imdb-accelerate\")\n",
    "    unwrapped_model.push_to_hub(model_name, token=token)\n",
    "    tokenizer.push_to_hub(model_name, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b670227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928206a4a6904fd39d5fd581ff573d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/500 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1569fa846ad479c9c2042965b926b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317e03465daf4674b52500d8dbee042a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071707fcab2742299a6f03683f3a4e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75514abc3bbd4b58a9df9158df1dabf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37154260fac44c5b68dbaeafb8dfaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: scale, Score: 0.5341\n",
      "Token: business, Score: 0.0183\n",
      "Token: size, Score: 0.0182\n",
      "Token: budget, Score: 0.0119\n",
      "Token: satellite, Score: 0.0079\n"
     ]
    }
   ],
   "source": [
    "# Reload our model and test\n",
    "from transformers import pipeline\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"tensor-polinomics/distilbert-finetuned-imdb-accelerate\",\n",
    ")\n",
    "\n",
    "preds = mask_filler(text)\n",
    "for pred in preds:\n",
    "    print(f\"Token: {pred['token_str']}, Score: {pred['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba684e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_llm",
   "language": "python",
   "name": "hf_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
