{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e097758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2293, 19081,   999,   102,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101, 17953,  2361,  2003,  1037,  7772,  2492,  1997,  3698,  4083,\n",
      "           102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "text = [\"I love transformers!\", \"NLP is a specialized field of machine learning\"]\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "# inputs is a dictionary containing input_ids and attention_mask\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb49e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  2293, 19081,   999,   102,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101, 17953,  2361,  2003,  1037,  7772,  2492,  1997,  3698,  4083,\n",
       "           102]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9608ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i love transformers! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] nlp is a specialized field of machine learning [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Convert input IDs back to text\n",
    "for i in range(len(text)):\n",
    "    print(tokenizer.decode(inputs[\"input_ids\"][i], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a04cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)  # (batch_size, sequence_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588f7bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)  # (batch_size, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1ac0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1216,  4.4303],\n",
      "        [-2.3793,  2.2820]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6ff745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9314e-04, 9.9981e-01],\n",
      "        [9.3661e-03, 9.9063e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pred_prob = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(pred_prob)\n",
    "predicted_classes = torch.argmax(pred_prob, dim=-1)\n",
    "print(predicted_classes) # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99dced",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e946020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fdd9a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a9271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1df7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./bert-base-uncased-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "835b8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"./bert-base-uncased-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be0fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[  101, 10938,  2121,  2003, 12476,   102]])\n",
      "token_type_ids: tensor([[0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_input = tokenizer(\"Transformer is awesome\", return_tensors=\"pt\")\n",
    "for key in encoded_input:\n",
    "    print(f\"{key}: {encoded_input[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af0fb3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 10938,  2121,  2003, 12476,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 17662,  2227,  2003,  4526,  1037,  6994,  2008,  7672, 10057,\n",
      "          9932,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\n",
    "    \"Transformer is awesome\", \"Hugging Face is creating a tool that democratizes AI.\"], \n",
    "                          padding=True, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49f294cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 10938,   102],\n",
      "        [  101, 17662,   102]]), 'token_type_ids': tensor([[0, 0, 0],\n",
      "        [0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\n",
    "    \"Transformer is awesome\", \n",
    "    \"Hugging Face is creating a tool that democratizes AI.\"], \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=3,\n",
    "    return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0d9fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 10938,  2121,  2003, 12476,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 17662,  2227,  2003,  4526,  1037,  6994,  2008,  7672, 10057,\n",
      "          9932,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\n",
    "    \"Transformer is awesome\", \"Hugging Face is creating a tool that democratizes AI.\"], \n",
    "                          padding=True, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d945521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 10938,  2121,  2003, 12476,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 17662,  2227,  2003,  4526,  1037,  6994,  2008,  7672, 10057,\n",
      "          9932,  1012,   102]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2856879/1223523145.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_tensor = torch.tensor(encoded_input_list)\n"
     ]
    }
   ],
   "source": [
    "encoded_input_list = encoded_input[\"input_ids\"]\n",
    "encoded_tensor = torch.tensor(encoded_input_list)\n",
    "print(encoded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c80be0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "output = model(encoded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a0a7fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'transformers!']\n"
     ]
    }
   ],
   "source": [
    "tokenizd_text = \"I love transformers!\".split()\n",
    "print(tokenizd_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23077bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5160872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101, 1045, 2293, 19081, 999, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(tokenizer(\"I love transformers!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb954488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hugging', 'face', 'is', 'creating', 'a', 'tool', 'that', 'democrat', '##izes', 'ai', '.']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Hugging Face is creating a tool that democratizes AI.\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a1abb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17662, 2227, 2003, 4526, 1037, 6994, 2008, 7672, 10057, 9932, 1012]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6cc6f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugging face is creating a tool that democratizes ai.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c59b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17662,  2227,  2003,  4526,  1037,  6994,  2008,  7672, 10057,  9932,\n",
      "          1012]])\n"
     ]
    }
   ],
   "source": [
    "ids_tensor = torch.tensor([ids])\n",
    "print(ids_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec937829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[ 2.4455, -2.2021]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "output = model(ids_tensor)\n",
    "print(f\"logits: {output.logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e571cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d66a5430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id]\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0]\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids),\n",
    "                attention_mask=torch.tensor(attention_mask))\n",
    "print(f\"logits: {outputs.logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2093581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 17662, 2227, 2003, 4526, 1037, 6994, 2008, 7672, 10057, 9932, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[101, 17662, 2227, 2003, 4526, 1037, 6994, 2008, 7672, 10057, 9932, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[101, 17662, 2227, 2003, 4526, 1037, 6994, 2008, 7672, 10057, 9932, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Hugging Face is creating a tool that democratizes AI.\"\n",
    "model_inputs_1 = tokenizer(sequence)\n",
    "print(model_inputs_1)\n",
    "\n",
    "model_inputs_2 = tokenizer([sequence], padding=\"longest\")\n",
    "print(model_inputs_2)\n",
    "\n",
    "model_inputs_3 = tokenizer([sequence], padding=\"max_length\", max_length=5)\n",
    "print(model_inputs_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e35ed3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "pprint.pprint(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87031be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i ' ve been waiting for a huggingface course my whole life. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb371f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761f3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
