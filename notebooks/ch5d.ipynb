{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e3100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve saved datasets\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Reload from saved file\n",
    "df = pd.read_json(\"../data/hf_github_issues/datasets-issues.jsonl\", lines=True)\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Reload comments dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "issues_with_comments_dataset = load_from_disk(\"../data/hf_github_issues/issues_with_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397be03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80d89cd10764361a5e1b71c97468776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03e76f6e1cd41ec8b5d03f470475085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef13cfc4911e4f23834793ee80a9677d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9897f9007e4044218496805c7495e027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tensor-polinomics/hf-datasets-github-issues-with-comments/commit/9140b3e90a78161925b8909b78607824ce673170', commit_message='Upload dataset', commit_description='', oid='9140b3e90a78161925b8909b78607824ce673170', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tensor-polinomics/hf-datasets-github-issues-with-comments', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tensor-polinomics/hf-datasets-github-issues-with-comments'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save dataset to the Hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "issues_with_comments_dataset.push_to_hub(\n",
    "    \"tensor-polinomics/hf-datasets-github-issues-with-comments\",\n",
    "    token=os.getenv(\"HF_TOKEN_WRITE\")  # Bypasses all cached credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8166a9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'issue_dependencies_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 7818\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "remote_dataset = load_dataset(\"tensor-polinomics/hf-datasets-github-issues-with-comments\", split=\"train\")\n",
    "remote_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5dffeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'issue_dependencies_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 2732\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove pull requests and rows with no comments\n",
    "issues_dataset = remote_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] is False) and (len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a075b0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2732\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove irrelevant columns\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "issues_dataset = issues_dataset.remove_columns(\n",
    "    [col for col in columns if col not in columns_to_keep]\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce03723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body': '## Summary\\n'\n",
      "         '\\n'\n",
      "         '`embed_table_storage` crashes with SIGKILL (exit code 137) when '\n",
      "         'processing sharded datasets containing `Sequence()` nested types '\n",
      "         'like `Sequence(Nifti())`. Likely affects `Sequence(Image())` and '\n",
      "         '`Sequence(Audio())` as well.\\n'\n",
      "         '\\n'\n",
      "         'The crash occurs at the C++ level with no Python traceback.\\n'\n",
      "         '\\n'\n",
      "         '### Related Issues\\n'\n",
      "         '\\n'\n",
      "         '- #7852 - Problems with NifTI (closed, but related embedding '\n",
      "         'issues)\\n'\n",
      "         \"- #6790 - PyArrow 'Memory mapping file failed' (potentially \"\n",
      "         'related)\\n'\n",
      "         '- #7893 - OOM issue (separate bug, but discovered together)\\n'\n",
      "         '\\n'\n",
      "         '### Context\\n'\n",
      "         '\\n'\n",
      "         'Discovered while uploading the [Aphasia Recovery Cohort '\n",
      "         '(ARC)](https://openneuro.org/datasets/ds004884) neuroimaging dataset '\n",
      "         'to HuggingFace Hub. Even after fixing the OOM issue (#7893), this '\n",
      "         'crash blocked uploads.\\n'\n",
      "         '\\n'\n",
      "         'Working implementation with workaround: '\n",
      "         '[arc-aphasia-bids](https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids)\\n'\n",
      "         '\\n'\n",
      "         '## Reproduction\\n'\n",
      "         '\\n'\n",
      "         '```python\\n'\n",
      "         'from datasets import Dataset, Features, Sequence, Value\\n'\n",
      "         'from datasets.features import Nifti\\n'\n",
      "         'from datasets.table import embed_table_storage\\n'\n",
      "         '\\n'\n",
      "         'features = Features({\\n'\n",
      "         '    \"id\": Value(\"string\"),\\n'\n",
      "         '    \"images\": Sequence(Nifti()),\\n'\n",
      "         '})\\n'\n",
      "         '\\n'\n",
      "         'ds = Dataset.from_dict({\\n'\n",
      "         '    \"id\": [\"a\", \"b\"],\\n'\n",
      "         '    \"images\": [[\"/path/to/file.nii.gz\"], []],\\n'\n",
      "         '}).cast(features)\\n'\n",
      "         '\\n'\n",
      "         '# This works fine:\\n'\n",
      "         'table = ds._data.table.combine_chunks()\\n'\n",
      "         'embedded = embed_table_storage(table)  # OK\\n'\n",
      "         '\\n'\n",
      "         '# This crashes with SIGKILL:\\n'\n",
      "         'shard = ds.shard(num_shards=2, index=0)\\n'\n",
      "         'shard_table = shard._data.table.combine_chunks()\\n'\n",
      "         'embedded = embed_table_storage(shard_table)  # CRASH - no Python '\n",
      "         'traceback\\n'\n",
      "         '```\\n'\n",
      "         '\\n'\n",
      "         '## Key Observations\\n'\n",
      "         '\\n'\n",
      "         '| Scenario | Result |\\n'\n",
      "         '|----------|--------|\\n'\n",
      "         '| Single `Nifti()` column | Works |\\n'\n",
      "         '| `Sequence(Nifti())` on full dataset | Works |\\n'\n",
      "         '| `Sequence(Nifti())` after `ds.shard()` | **CRASHES** |\\n'\n",
      "         '| `Sequence(Nifti())` after `ds.select([i])` | **CRASHES** |\\n'\n",
      "         '| Crash with empty Sequence `[]` | **YES** - not file-size related '\n",
      "         '|\\n'\n",
      "         '\\n'\n",
      "         '## Workaround\\n'\n",
      "         '\\n'\n",
      "         'Convert shard to pandas and recreate the Dataset to break internal '\n",
      "         'Arrow references:\\n'\n",
      "         '\\n'\n",
      "         '```python\\n'\n",
      "         'shard = ds.shard(num_shards=num_shards, index=i, contiguous=True)\\n'\n",
      "         '\\n'\n",
      "         '# CRITICAL: Pandas round-trip breaks problematic references\\n'\n",
      "         'shard_df = shard.to_pandas()\\n'\n",
      "         'fresh_shard = Dataset.from_pandas(shard_df, preserve_index=False)\\n'\n",
      "         'fresh_shard = fresh_shard.cast(ds.features)\\n'\n",
      "         '\\n'\n",
      "         '# Now embedding works\\n'\n",
      "         'table = fresh_shard._data.table.combine_chunks()\\n'\n",
      "         'embedded = embed_table_storage(table)  # OK!\\n'\n",
      "         '```\\n'\n",
      "         '\\n'\n",
      "         '## Disproven Hypotheses\\n'\n",
      "         '\\n'\n",
      "         '| Hypothesis | Test | Result |\\n'\n",
      "         '|------------|------|--------|\\n'\n",
      "         '| PyArrow 2GB binary limit | Monkey-patched `Nifti.pa_type` to '\n",
      "         '`pa.large_binary()` | Still crashed |\\n'\n",
      "         '| Memory fragmentation | Called `table.combine_chunks()` | Still '\n",
      "         'crashed |\\n'\n",
      "         '| File size issue | Tested with tiny NIfTI files | Still crashed |\\n'\n",
      "         '\\n'\n",
      "         '## Root Cause Hypothesis\\n'\n",
      "         '\\n'\n",
      "         'When `ds.shard()` or `ds.select()` creates a subset, the resulting '\n",
      "         'Arrow table retains internal references/views to the parent table. '\n",
      "         'When `embed_table_storage` processes nested struct types like '\n",
      "         '`Sequence(Nifti())`, these references cause a crash in the C++ '\n",
      "         'layer.\\n'\n",
      "         '\\n'\n",
      "         'The pandas round-trip forces a full data copy, breaking these '\n",
      "         'problematic references.\\n'\n",
      "         '\\n'\n",
      "         '## Environment\\n'\n",
      "         '\\n'\n",
      "         '- datasets version: main branch (post-0.22.0)\\n'\n",
      "         '- Platform: macOS 14.x ARM64 (may be platform-specific)\\n'\n",
      "         '- Python: 3.13\\n'\n",
      "         '- PyArrow: 18.1.0\\n'\n",
      "         '\\n'\n",
      "         '## Notes\\n'\n",
      "         '\\n'\n",
      "         'This may ultimately be a PyArrow issue surfacing through datasets. '\n",
      "         'Happy to help debug further if maintainers can point to where to '\n",
      "         'look in the embedding logic.',\n",
      " 'comments': [\"I wasn't able to reproduce the crash on my side (macos arm 54, \"\n",
      "              'pyarrow 22 and a nifti file I found '\n",
      "              '[online](https://s3.amazonaws.com/openneuro.org/ds004884/sub-M2001/ses-1076/anat/sub-M2001_ses-1076_acq-tfl3_run-4_T1w.nii.gz?versionId=9aVGb3C.VcoBgxrhNzFnL6O0MvxQsXX7&AWSAccessKeyId=AKIARTA7OOV5WQ3DGSOB&Signature=LQMLzjsuzSV7MtNAdQaFdqWqmbM%3D&Expires=1765473937))\\n'\n",
      "              '\\n'\n",
      "              'could the issue be specific to your env ? have you tried on '\n",
      "              'other environments like colab maybe ?',\n",
      "              'Hi @lhoestq,\\n'\n",
      "              '\\n'\n",
      "              'Thank you so much for taking the time to investigate this. Your '\n",
      "              'comment about not being able to reproduce it with a single '\n",
      "              'NIfTI file actually helped me understand the bug better.\\n'\n",
      "              '\\n'\n",
      "              '**Key finding:** This bug is scale-dependent. It only manifests '\n",
      "              'with real, full-scale data, and not with synthetic test files.\\n'\n",
      "              '\\n'\n",
      "              'I created a sandbox branch that isolates the exact state before '\n",
      "              'the workaround:\\n'\n",
      "              '\\n'\n",
      "              '**üîó Reproduction branch:** '\n",
      "              'https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids/tree/sandbox/reproduce-bug-7894\\n'\n",
      "              '\\n'\n",
      "              '### What we confirmed\\n'\n",
      "              '\\n'\n",
      "              '| Test | Result |\\n'\n",
      "              '|------|--------|\\n'\n",
      "              '| Synthetic 2x2x2 NIfTI files | ‚úÖ No crash |\\n'\n",
      "              '| Synthetic 64¬≥ NIfTI files (1MB each) | ‚úÖ No crash |\\n'\n",
      "              '| Real ARC dataset (273GB, 902 sessions) | ‚ùå **SIGKILL at 0%** '\n",
      "              '|\\n'\n",
      "              '\\n'\n",
      "              '### Environment (same as yours)\\n'\n",
      "              '\\n'\n",
      "              '- macOS ARM64\\n'\n",
      "              '- PyArrow 22.0.0\\n'\n",
      "              '- datasets 4.4.2.dev0 (git main)\\n'\n",
      "              '\\n'\n",
      "              '### Crash output\\n'\n",
      "              '\\n'\n",
      "              '```\\n'\n",
      "              'Casting the dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 902/902\\n'\n",
      "              'Uploading Shards:   0%|          | 0/902\\n'\n",
      "              'UserWarning: resource_tracker: There appear to be 1 leaked '\n",
      "              'semaphore objects\\n'\n",
      "              '```\\n'\n",
      "              '**Exit code: 137 (SIGKILL)**\\n'\n",
      "              '\\n'\n",
      "              'The crash happens on the very first shard, at '\n",
      "              '`embed_table_storage()`, when processing `Sequence(Nifti())` '\n",
      "              'columns after `ds.shard()`.\\n'\n",
      "              '\\n'\n",
      "              '### The workaround (in main branch)\\n'\n",
      "              '\\n'\n",
      "              'A pandas round-trip before embedding breaks the problematic '\n",
      "              'Arrow references:\\n'\n",
      "              '\\n'\n",
      "              '```python\\n'\n",
      "              'shard_df = shard.to_pandas()\\n'\n",
      "              'fresh_shard = Dataset.from_pandas(shard_df, '\n",
      "              'preserve_index=False)\\n'\n",
      "              'fresh_shard = fresh_shard.cast(ds.features)\\n'\n",
      "              '# Now embed_table_storage works\\n'\n",
      "              '```\\n'\n",
      "              '\\n'\n",
      "              \"We understand that downloading 273GB to reproduce this isn't \"\n",
      "              'practical. The reproduction guide in the branch has full '\n",
      "              \"details if you'd like to dig deeper. Happy to help debug \"\n",
      "              'further if useful.\\n'\n",
      "              '\\n'\n",
      "              'Thank you again for your time and for maintaining this '\n",
      "              'library. ',\n",
      "              \"@lhoestq Brief update - I've added a reproduction that uses \"\n",
      "              'standard `ds.push_to_hub()` (no custom code).\\n'\n",
      "              '\\n'\n",
      "              '**Reproduction branch:** '\n",
      "              'https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids/tree/sandbox/reproduce-bug-7894\\n'\n",
      "              '\\n'\n",
      "              '**To reproduce with standard library:**\\n'\n",
      "              '```bash\\n'\n",
      "              'git clone -b sandbox/reproduce-bug-7894 '\n",
      "              'https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids.git\\n'\n",
      "              'cd arc-aphasia-bids && uv sync --all-extras\\n'\n",
      "              '# Download dataset (~273GB): aws s3 sync --no-sign-request '\n",
      "              's3://openneuro.org/ds004884 data/openneuro/ds004884\\n'\n",
      "              'HF_REPO=\"your-username/test\" uv run python '\n",
      "              'test_prove_7894_standard.py\\n'\n",
      "              '```\\n'\n",
      "              '\\n'\n",
      "              'Crashes at 0% with the same semaphore warning.\\n'\n",
      "              '\\n'\n",
      "              'Full details in '\n",
      "              '[REPRODUCE_BUG_7894.md](https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids/blob/sandbox/reproduce-bug-7894/REPRODUCE_BUG_7894.md).\\n'\n",
      "              '\\n'\n",
      "              'Also - you were right about #7893. I closed it. '\n",
      "              '`free_memory=True` works as you said. That issue was my '\n",
      "              'mistake.'],\n",
      " 'html_url': 'https://github.com/huggingface/datasets/issues/7894',\n",
      " 'title': 'embed_table_storage crashes (SIGKILL) on sharded datasets with '\n",
      "          'Sequence() nested types'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(issues_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to pandas for data processing\n",
    "issues_dataset.set_format(\"pandas\")\n",
    "df_issues = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71816ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I wasn't able to reproduce the crash on my side (macos arm 54, pyarrow 22 \"\n",
      " 'and a nifti file I found '\n",
      " '[online](https://s3.amazonaws.com/openneuro.org/ds004884/sub-M2001/ses-1076/anat/sub-M2001_ses-1076_acq-tfl3_run-4_T1w.nii.gz?versionId=9aVGb3C.VcoBgxrhNzFnL6O0MvxQsXX7&AWSAccessKeyId=AKIARTA7OOV5WQ3DGSOB&Signature=LQMLzjsuzSV7MtNAdQaFdqWqmbM%3D&Expires=1765473937))\\n'\n",
      " '\\n'\n",
      " 'could the issue be specific to your env ? have you tried on other '\n",
      " 'environments like colab maybe ?',\n",
      " 'Hi @lhoestq,\\n'\n",
      " '\\n'\n",
      " 'Thank you so much for taking the time to investigate this. Your comment '\n",
      " 'about not being able to reproduce it with a single NIfTI file actually '\n",
      " 'helped me understand the bug better.\\n'\n",
      " '\\n'\n",
      " '**Key finding:** This bug is scale-dependent. It only manifests with real, '\n",
      " 'full-scale data, and not with synthetic test files.\\n'\n",
      " '\\n'\n",
      " 'I created a sandbox branch that isolates the exact state before the '\n",
      " 'workaround:\\n'\n",
      " '\\n'\n",
      " '**üîó Reproduction branch:** '\n",
      " 'https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids/tree/sandbox/reproduce-bug-7894\\n'\n",
      " '\\n'\n",
      " '### What we confirmed\\n'\n",
      " '\\n'\n",
      " '| Test | Result |\\n'\n",
      " '|------|--------|\\n'\n",
      " '| Synthetic 2x2x2 NIfTI files | ‚úÖ No crash |\\n'\n",
      " '| Synthetic 64¬≥ NIfTI files (1MB each) | ‚úÖ No crash |\\n'\n",
      " '| Real ARC dataset (273GB, 902 sessions) | ‚ùå **SIGKILL at 0%** |\\n'\n",
      " '\\n'\n",
      " '### Environment (same as yours)\\n'\n",
      " '\\n'\n",
      " '- macOS ARM64\\n'\n",
      " '- PyArrow 22.0.0\\n'\n",
      " '- datasets 4.4.2.dev0 (git main)\\n'\n",
      " '\\n'\n",
      " '### Crash output\\n'\n",
      " '\\n'\n",
      " '```\\n'\n",
      " 'Casting the dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 902/902\\n'\n",
      " 'Uploading Shards:   0%|          | 0/902\\n'\n",
      " 'UserWarning: resource_tracker: There appear to be 1 leaked semaphore '\n",
      " 'objects\\n'\n",
      " '```\\n'\n",
      " '**Exit code: 137 (SIGKILL)**\\n'\n",
      " '\\n'\n",
      " 'The crash happens on the very first shard, at `embed_table_storage()`, when '\n",
      " 'processing `Sequence(Nifti())` columns after `ds.shard()`.\\n'\n",
      " '\\n'\n",
      " '### The workaround (in main branch)\\n'\n",
      " '\\n'\n",
      " 'A pandas round-trip before embedding breaks the problematic Arrow '\n",
      " 'references:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'shard_df = shard.to_pandas()\\n'\n",
      " 'fresh_shard = Dataset.from_pandas(shard_df, preserve_index=False)\\n'\n",
      " 'fresh_shard = fresh_shard.cast(ds.features)\\n'\n",
      " '# Now embed_table_storage works\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " \"We understand that downloading 273GB to reproduce this isn't practical. The \"\n",
      " \"reproduction guide in the branch has full details if you'd like to dig \"\n",
      " 'deeper. Happy to help debug further if useful.\\n'\n",
      " '\\n'\n",
      " 'Thank you again for your time and for maintaining this library. ',\n",
      " \"@lhoestq Brief update - I've added a reproduction that uses standard \"\n",
      " '`ds.push_to_hub()` (no custom code).\\n'\n",
      " '\\n'\n",
      " '**Reproduction branch:** '\n",
      " 'https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids/tree/sandbox/reproduce-bug-7894\\n'\n",
      " '\\n'\n",
      " '**To reproduce with standard library:**\\n'\n",
      " '```bash\\n'\n",
      " 'git clone -b sandbox/reproduce-bug-7894 '\n",
      " 'https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids.git\\n'\n",
      " 'cd arc-aphasia-bids && uv sync --all-extras\\n'\n",
      " '# Download dataset (~273GB): aws s3 sync --no-sign-request '\n",
      " 's3://openneuro.org/ds004884 data/openneuro/ds004884\\n'\n",
      " 'HF_REPO=\"your-username/test\" uv run python test_prove_7894_standard.py\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Crashes at 0% with the same semaphore warning.\\n'\n",
      " '\\n'\n",
      " 'Full details in '\n",
      " '[REPRODUCE_BUG_7894.md](https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids/blob/sandbox/reproduce-bug-7894/REPRODUCE_BUG_7894.md).\\n'\n",
      " '\\n'\n",
      " 'Also - you were right about #7893. I closed it. `free_memory=True` works as '\n",
      " 'you said. That issue was my mistake.']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(df_issues[\"comments\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b4de2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>embed_table_storage crashes (SIGKILL) on shard...</td>\n",
       "      <td>I wasn't able to reproduce the crash on my sid...</td>\n",
       "      <td>## Summary\\n\\n`embed_table_storage` crashes wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>embed_table_storage crashes (SIGKILL) on shard...</td>\n",
       "      <td>Hi @lhoestq,\\n\\nThank you so much for taking t...</td>\n",
       "      <td>## Summary\\n\\n`embed_table_storage` crashes wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>embed_table_storage crashes (SIGKILL) on shard...</td>\n",
       "      <td>@lhoestq Brief update - I've added a reproduct...</td>\n",
       "      <td>## Summary\\n\\n`embed_table_storage` crashes wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>push_to_hub OOM: _push_parquet_shards_to_hub a...</td>\n",
       "      <td>`preupload_lfs_files` removes the parquet byte...</td>\n",
       "      <td>## Summary\\n\\nLarge dataset uploads crash or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>push_to_hub OOM: _push_parquet_shards_to_hub a...</td>\n",
       "      <td>@lhoestq Thank you for pushing back on this an...</td>\n",
       "      <td>## Summary\\n\\nLarge dataset uploads crash or h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0  embed_table_storage crashes (SIGKILL) on shard...   \n",
       "1  embed_table_storage crashes (SIGKILL) on shard...   \n",
       "2  embed_table_storage crashes (SIGKILL) on shard...   \n",
       "3  push_to_hub OOM: _push_parquet_shards_to_hub a...   \n",
       "4  push_to_hub OOM: _push_parquet_shards_to_hub a...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  I wasn't able to reproduce the crash on my sid...   \n",
       "1  Hi @lhoestq,\\n\\nThank you so much for taking t...   \n",
       "2  @lhoestq Brief update - I've added a reproduct...   \n",
       "3  `preupload_lfs_files` removes the parquet byte...   \n",
       "4  @lhoestq Thank you for pushing back on this an...   \n",
       "\n",
       "                                                body  \n",
       "0  ## Summary\\n\\n`embed_table_storage` crashes wi...  \n",
       "1  ## Summary\\n\\n`embed_table_storage` crashes wi...  \n",
       "2  ## Summary\\n\\n`embed_table_storage` crashes wi...  \n",
       "3  ## Summary\\n\\nLarge dataset uploads crash or h...  \n",
       "4  ## Summary\\n\\nLarge dataset uploads crash or h...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issues_full = df_issues.explode(\"comments\", ignore_index=True)\n",
    "df_issues_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d724a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 10498\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch back to Dataset\n",
    "from datasets import Dataset\n",
    "comments_dataset = Dataset.from_pandas(df_issues_full)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "283d1219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a1a34e3bc04defabc352926ea6af2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 10498\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column containing the number of words per comment\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba5de49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/7894',\n",
       " 'title': 'embed_table_storage crashes (SIGKILL) on sharded datasets with Sequence() nested types',\n",
       " 'comments': \"I wasn't able to reproduce the crash on my side (macos arm 54, pyarrow 22 and a nifti file I found [online](https://s3.amazonaws.com/openneuro.org/ds004884/sub-M2001/ses-1076/anat/sub-M2001_ses-1076_acq-tfl3_run-4_T1w.nii.gz?versionId=9aVGb3C.VcoBgxrhNzFnL6O0MvxQsXX7&AWSAccessKeyId=AKIARTA7OOV5WQ3DGSOB&Signature=LQMLzjsuzSV7MtNAdQaFdqWqmbM%3D&Expires=1765473937))\\n\\ncould the issue be specific to your env ? have you tried on other environments like colab maybe ?\",\n",
       " 'body': '## Summary\\n\\n`embed_table_storage` crashes with SIGKILL (exit code 137) when processing sharded datasets containing `Sequence()` nested types like `Sequence(Nifti())`. Likely affects `Sequence(Image())` and `Sequence(Audio())` as well.\\n\\nThe crash occurs at the C++ level with no Python traceback.\\n\\n### Related Issues\\n\\n- #7852 - Problems with NifTI (closed, but related embedding issues)\\n- #6790 - PyArrow \\'Memory mapping file failed\\' (potentially related)\\n- #7893 - OOM issue (separate bug, but discovered together)\\n\\n### Context\\n\\nDiscovered while uploading the [Aphasia Recovery Cohort (ARC)](https://openneuro.org/datasets/ds004884) neuroimaging dataset to HuggingFace Hub. Even after fixing the OOM issue (#7893), this crash blocked uploads.\\n\\nWorking implementation with workaround: [arc-aphasia-bids](https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids)\\n\\n## Reproduction\\n\\n```python\\nfrom datasets import Dataset, Features, Sequence, Value\\nfrom datasets.features import Nifti\\nfrom datasets.table import embed_table_storage\\n\\nfeatures = Features({\\n    \"id\": Value(\"string\"),\\n    \"images\": Sequence(Nifti()),\\n})\\n\\nds = Dataset.from_dict({\\n    \"id\": [\"a\", \"b\"],\\n    \"images\": [[\"/path/to/file.nii.gz\"], []],\\n}).cast(features)\\n\\n# This works fine:\\ntable = ds._data.table.combine_chunks()\\nembedded = embed_table_storage(table)  # OK\\n\\n# This crashes with SIGKILL:\\nshard = ds.shard(num_shards=2, index=0)\\nshard_table = shard._data.table.combine_chunks()\\nembedded = embed_table_storage(shard_table)  # CRASH - no Python traceback\\n```\\n\\n## Key Observations\\n\\n| Scenario | Result |\\n|----------|--------|\\n| Single `Nifti()` column | Works |\\n| `Sequence(Nifti())` on full dataset | Works |\\n| `Sequence(Nifti())` after `ds.shard()` | **CRASHES** |\\n| `Sequence(Nifti())` after `ds.select([i])` | **CRASHES** |\\n| Crash with empty Sequence `[]` | **YES** - not file-size related |\\n\\n## Workaround\\n\\nConvert shard to pandas and recreate the Dataset to break internal Arrow references:\\n\\n```python\\nshard = ds.shard(num_shards=num_shards, index=i, contiguous=True)\\n\\n# CRITICAL: Pandas round-trip breaks problematic references\\nshard_df = shard.to_pandas()\\nfresh_shard = Dataset.from_pandas(shard_df, preserve_index=False)\\nfresh_shard = fresh_shard.cast(ds.features)\\n\\n# Now embedding works\\ntable = fresh_shard._data.table.combine_chunks()\\nembedded = embed_table_storage(table)  # OK!\\n```\\n\\n## Disproven Hypotheses\\n\\n| Hypothesis | Test | Result |\\n|------------|------|--------|\\n| PyArrow 2GB binary limit | Monkey-patched `Nifti.pa_type` to `pa.large_binary()` | Still crashed |\\n| Memory fragmentation | Called `table.combine_chunks()` | Still crashed |\\n| File size issue | Tested with tiny NIfTI files | Still crashed |\\n\\n## Root Cause Hypothesis\\n\\nWhen `ds.shard()` or `ds.select()` creates a subset, the resulting Arrow table retains internal references/views to the parent table. When `embed_table_storage` processes nested struct types like `Sequence(Nifti())`, these references cause a crash in the C++ layer.\\n\\nThe pandas round-trip forces a full data copy, breaking these problematic references.\\n\\n## Environment\\n\\n- datasets version: main branch (post-0.22.0)\\n- Platform: macOS 14.x ARM64 (may be platform-specific)\\n- Python: 3.13\\n- PyArrow: 18.1.0\\n\\n## Notes\\n\\nThis may ultimately be a PyArrow issue surfacing through datasets. Happy to help debug further if maintainers can point to where to look in the embedding logic.',\n",
       " 'comment_length': 41}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79991c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417e5e71d2d3418db3ba7671b1aafce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 8050\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where comment_length < 14\n",
    "comments_dataset = comments_dataset.filter(\n",
    "    lambda x: x[\"comment_length\"] >= 14\n",
    ")\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12dd2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(examples):\n",
    "    title = examples[\"title\"] or \"\"\n",
    "    body = examples[\"body\"] or \"\"\n",
    "    comments = examples[\"comments\"] or \"\"\n",
    "    \n",
    "    # If comments is a list, join it\n",
    "    if isinstance(comments, list):\n",
    "        comments = \" \".join(comments)\n",
    "    \n",
    "    return {\"text\": f\"{title} \\n {body} \\n {comments}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe392b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346e727de88d4c439b7b7c1aaf23b192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8050 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 8050\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the dataset\n",
    "comments_dataset = comments_dataset.map(concatenate_text)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ea5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/7894',\n",
       " 'title': 'embed_table_storage crashes (SIGKILL) on sharded datasets with Sequence() nested types',\n",
       " 'comments': \"I wasn't able to reproduce the crash on my side (macos arm 54, pyarrow 22 and a nifti file I found [online](https://s3.amazonaws.com/openneuro.org/ds004884/sub-M2001/ses-1076/anat/sub-M2001_ses-1076_acq-tfl3_run-4_T1w.nii.gz?versionId=9aVGb3C.VcoBgxrhNzFnL6O0MvxQsXX7&AWSAccessKeyId=AKIARTA7OOV5WQ3DGSOB&Signature=LQMLzjsuzSV7MtNAdQaFdqWqmbM%3D&Expires=1765473937))\\n\\ncould the issue be specific to your env ? have you tried on other environments like colab maybe ?\",\n",
       " 'body': '## Summary\\n\\n`embed_table_storage` crashes with SIGKILL (exit code 137) when processing sharded datasets containing `Sequence()` nested types like `Sequence(Nifti())`. Likely affects `Sequence(Image())` and `Sequence(Audio())` as well.\\n\\nThe crash occurs at the C++ level with no Python traceback.\\n\\n### Related Issues\\n\\n- #7852 - Problems with NifTI (closed, but related embedding issues)\\n- #6790 - PyArrow \\'Memory mapping file failed\\' (potentially related)\\n- #7893 - OOM issue (separate bug, but discovered together)\\n\\n### Context\\n\\nDiscovered while uploading the [Aphasia Recovery Cohort (ARC)](https://openneuro.org/datasets/ds004884) neuroimaging dataset to HuggingFace Hub. Even after fixing the OOM issue (#7893), this crash blocked uploads.\\n\\nWorking implementation with workaround: [arc-aphasia-bids](https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids)\\n\\n## Reproduction\\n\\n```python\\nfrom datasets import Dataset, Features, Sequence, Value\\nfrom datasets.features import Nifti\\nfrom datasets.table import embed_table_storage\\n\\nfeatures = Features({\\n    \"id\": Value(\"string\"),\\n    \"images\": Sequence(Nifti()),\\n})\\n\\nds = Dataset.from_dict({\\n    \"id\": [\"a\", \"b\"],\\n    \"images\": [[\"/path/to/file.nii.gz\"], []],\\n}).cast(features)\\n\\n# This works fine:\\ntable = ds._data.table.combine_chunks()\\nembedded = embed_table_storage(table)  # OK\\n\\n# This crashes with SIGKILL:\\nshard = ds.shard(num_shards=2, index=0)\\nshard_table = shard._data.table.combine_chunks()\\nembedded = embed_table_storage(shard_table)  # CRASH - no Python traceback\\n```\\n\\n## Key Observations\\n\\n| Scenario | Result |\\n|----------|--------|\\n| Single `Nifti()` column | Works |\\n| `Sequence(Nifti())` on full dataset | Works |\\n| `Sequence(Nifti())` after `ds.shard()` | **CRASHES** |\\n| `Sequence(Nifti())` after `ds.select([i])` | **CRASHES** |\\n| Crash with empty Sequence `[]` | **YES** - not file-size related |\\n\\n## Workaround\\n\\nConvert shard to pandas and recreate the Dataset to break internal Arrow references:\\n\\n```python\\nshard = ds.shard(num_shards=num_shards, index=i, contiguous=True)\\n\\n# CRITICAL: Pandas round-trip breaks problematic references\\nshard_df = shard.to_pandas()\\nfresh_shard = Dataset.from_pandas(shard_df, preserve_index=False)\\nfresh_shard = fresh_shard.cast(ds.features)\\n\\n# Now embedding works\\ntable = fresh_shard._data.table.combine_chunks()\\nembedded = embed_table_storage(table)  # OK!\\n```\\n\\n## Disproven Hypotheses\\n\\n| Hypothesis | Test | Result |\\n|------------|------|--------|\\n| PyArrow 2GB binary limit | Monkey-patched `Nifti.pa_type` to `pa.large_binary()` | Still crashed |\\n| Memory fragmentation | Called `table.combine_chunks()` | Still crashed |\\n| File size issue | Tested with tiny NIfTI files | Still crashed |\\n\\n## Root Cause Hypothesis\\n\\nWhen `ds.shard()` or `ds.select()` creates a subset, the resulting Arrow table retains internal references/views to the parent table. When `embed_table_storage` processes nested struct types like `Sequence(Nifti())`, these references cause a crash in the C++ layer.\\n\\nThe pandas round-trip forces a full data copy, breaking these problematic references.\\n\\n## Environment\\n\\n- datasets version: main branch (post-0.22.0)\\n- Platform: macOS 14.x ARM64 (may be platform-specific)\\n- Python: 3.13\\n- PyArrow: 18.1.0\\n\\n## Notes\\n\\nThis may ultimately be a PyArrow issue surfacing through datasets. Happy to help debug further if maintainers can point to where to look in the embedding logic.',\n",
       " 'comment_length': 41,\n",
       " 'combined_text': 'Title: embed_table_storage crashes (SIGKILL) on sharded datasets with Sequence() nested types\\nBody: ## Summary\\n\\n`embed_table_storage` crashes with SIGKILL (exit code 137) when processing sharded datasets containing `Sequence()` nested types like `Sequence(Nifti())`. Likely affects `Sequence(Image())` and `Sequence(Audio())` as well.\\n\\nThe crash occurs at the C++ level with no Python traceback.\\n\\n### Related Issues\\n\\n- #7852 - Problems with NifTI (closed, but related embedding issues)\\n- #6790 - PyArrow \\'Memory mapping file failed\\' (potentially related)\\n- #7893 - OOM issue (separate bug, but discovered together)\\n\\n### Context\\n\\nDiscovered while uploading the [Aphasia Recovery Cohort (ARC)](https://openneuro.org/datasets/ds004884) neuroimaging dataset to HuggingFace Hub. Even after fixing the OOM issue (#7893), this crash blocked uploads.\\n\\nWorking implementation with workaround: [arc-aphasia-bids](https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids)\\n\\n## Reproduction\\n\\n```python\\nfrom datasets import Dataset, Features, Sequence, Value\\nfrom datasets.features import Nifti\\nfrom datasets.table import embed_table_storage\\n\\nfeatures = Features({\\n    \"id\": Value(\"string\"),\\n    \"images\": Sequence(Nifti()),\\n})\\n\\nds = Dataset.from_dict({\\n    \"id\": [\"a\", \"b\"],\\n    \"images\": [[\"/path/to/file.nii.gz\"], []],\\n}).cast(features)\\n\\n# This works fine:\\ntable = ds._data.table.combine_chunks()\\nembedded = embed_table_storage(table)  # OK\\n\\n# This crashes with SIGKILL:\\nshard = ds.shard(num_shards=2, index=0)\\nshard_table = shard._data.table.combine_chunks()\\nembedded = embed_table_storage(shard_table)  # CRASH - no Python traceback\\n```\\n\\n## Key Observations\\n\\n| Scenario | Result |\\n|----------|--------|\\n| Single `Nifti()` column | Works |\\n| `Sequence(Nifti())` on full dataset | Works |\\n| `Sequence(Nifti())` after `ds.shard()` | **CRASHES** |\\n| `Sequence(Nifti())` after `ds.select([i])` | **CRASHES** |\\n| Crash with empty Sequence `[]` | **YES** - not file-size related |\\n\\n## Workaround\\n\\nConvert shard to pandas and recreate the Dataset to break internal Arrow references:\\n\\n```python\\nshard = ds.shard(num_shards=num_shards, index=i, contiguous=True)\\n\\n# CRITICAL: Pandas round-trip breaks problematic references\\nshard_df = shard.to_pandas()\\nfresh_shard = Dataset.from_pandas(shard_df, preserve_index=False)\\nfresh_shard = fresh_shard.cast(ds.features)\\n\\n# Now embedding works\\ntable = fresh_shard._data.table.combine_chunks()\\nembedded = embed_table_storage(table)  # OK!\\n```\\n\\n## Disproven Hypotheses\\n\\n| Hypothesis | Test | Result |\\n|------------|------|--------|\\n| PyArrow 2GB binary limit | Monkey-patched `Nifti.pa_type` to `pa.large_binary()` | Still crashed |\\n| Memory fragmentation | Called `table.combine_chunks()` | Still crashed |\\n| File size issue | Tested with tiny NIfTI files | Still crashed |\\n\\n## Root Cause Hypothesis\\n\\nWhen `ds.shard()` or `ds.select()` creates a subset, the resulting Arrow table retains internal references/views to the parent table. When `embed_table_storage` processes nested struct types like `Sequence(Nifti())`, these references cause a crash in the C++ layer.\\n\\nThe pandas round-trip forces a full data copy, breaking these problematic references.\\n\\n## Environment\\n\\n- datasets version: main branch (post-0.22.0)\\n- Platform: macOS 14.x ARM64 (may be platform-specific)\\n- Python: 3.13\\n- PyArrow: 18.1.0\\n\\n## Notes\\n\\nThis may ultimately be a PyArrow issue surfacing through datasets. Happy to help debug further if maintainers can point to where to look in the embedding logic.\\nComment: I wasn\\'t able to reproduce the crash on my side (macos arm 54, pyarrow 22 and a nifti file I found [online](https://s3.amazonaws.com/openneuro.org/ds004884/sub-M2001/ses-1076/anat/sub-M2001_ses-1076_acq-tfl3_run-4_T1w.nii.gz?versionId=9aVGb3C.VcoBgxrhNzFnL6O0MvxQsXX7&AWSAccessKeyId=AKIARTA7OOV5WQ3DGSOB&Signature=LQMLzjsuzSV7MtNAdQaFdqWqmbM%3D&Expires=1765473937))\\n\\ncould the issue be specific to your env ? have you tried on other environments like colab maybe ?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46161eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68173d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a pooling function, using the last hidden state from CLS token\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30250978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to compute embeddings\n",
    "def get_embedding(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e72534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embedding(comments_dataset[\"text\"][:10])\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a64523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
