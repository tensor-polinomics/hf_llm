{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in zip: ['metadata', 'test.txt', 'train.txt', 'valid.txt']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed93b53a97054050aeb5e9ed4d6bb4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcec80b22904cceb0a21777e4e90964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9ac7f1f1284de49640009a8a525ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The original data is no longer supported. It takes a lot more efforts to reconstruct the data.\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence, Features, Value\n",
    "\n",
    "# Download from DeepAI\n",
    "url = \"https://data.deepai.org/conll2003.zip\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Extract zip in memory\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    print(\"Files in zip:\", z.namelist())\n",
    "    train_text = z.read(\"train.txt\").decode(\"utf-8\")\n",
    "    valid_text = z.read(\"valid.txt\").decode(\"utf-8\")\n",
    "    test_text = z.read(\"test.txt\").decode(\"utf-8\")\n",
    "\n",
    "def parse_conll(text):\n",
    "    \"\"\"Parse CoNLL format into all fields: tokens, pos_tags, chunk_tags, ner_tags.\"\"\"\n",
    "    sentences = []\n",
    "    tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "    idx = 0\n",
    "    \n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        if line.startswith(\"-DOCSTART-\") or line == \"\":\n",
    "            if tokens:\n",
    "                sentences.append({\n",
    "                    \"id\": str(idx),\n",
    "                    \"tokens\": tokens,\n",
    "                    \"pos_tags\": pos_tags,\n",
    "                    \"chunk_tags\": chunk_tags,\n",
    "                    \"ner_tags\": ner_tags,\n",
    "                })\n",
    "                tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "                idx += 1\n",
    "        else:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4:\n",
    "                tokens.append(parts[0])\n",
    "                pos_tags.append(parts[1])\n",
    "                chunk_tags.append(parts[2])\n",
    "                ner_tags.append(parts[3])\n",
    "    \n",
    "    if tokens:\n",
    "        sentences.append({\n",
    "            \"id\": str(idx),\n",
    "            \"tokens\": tokens,\n",
    "            \"pos_tags\": pos_tags,\n",
    "            \"chunk_tags\": chunk_tags,\n",
    "            \"ner_tags\": ner_tags,\n",
    "        })\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Parse all splits\n",
    "splits = {\n",
    "    \"train\": parse_conll(train_text),\n",
    "    \"validation\": parse_conll(valid_text),\n",
    "    \"test\": parse_conll(test_text),\n",
    "}\n",
    "\n",
    "# Define label mappings (matching HF's eriktks/conll2003)\n",
    "ner_labels = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "\n",
    "pos_labels = ['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', \n",
    "              'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', \n",
    "              'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', \n",
    "              'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', \n",
    "              'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "chunk_labels = ['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', \n",
    "                'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', \n",
    "                'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n",
    "\n",
    "ner_label2id = {label: i for i, label in enumerate(ner_labels)}\n",
    "pos_label2id = {label: i for i, label in enumerate(pos_labels)}\n",
    "chunk_label2id = {label: i for i, label in enumerate(chunk_labels)}\n",
    "\n",
    "# Convert string tags to IDs\n",
    "for split in splits:\n",
    "    for item in splits[split]:\n",
    "        item[\"ner_tags\"] = [ner_label2id.get(tag, 0) for tag in item[\"ner_tags\"]]\n",
    "        item[\"pos_tags\"] = [pos_label2id.get(tag, 0) for tag in item[\"pos_tags\"]]\n",
    "        item[\"chunk_tags\"] = [chunk_label2id.get(tag, 0) for tag in item[\"chunk_tags\"]]\n",
    "\n",
    "# Create HF Dataset with proper features\n",
    "features = Features({\n",
    "    \"id\": Value(\"string\"),\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"pos_tags\": Sequence(ClassLabel(names=pos_labels)),\n",
    "    \"chunk_tags\": Sequence(ClassLabel(names=chunk_labels)),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=ner_labels)),\n",
    "})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    split: Dataset.from_list(data, features=features)\n",
    "    for split, data in splits.items()\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# Save locally for future use\n",
    "dataset.save_to_disk(\"../data/conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7967d304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06e19cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = dataset[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed72bceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eabc8cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer . \n",
      "B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O \n"
     ]
    }
   ],
   "source": [
    "words = dataset[\"train\"][4][\"tokens\"]\n",
    "labels = dataset[\"train\"][4][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_len = max(len(word), len(full_label))\n",
    "    line1 += word.ljust(max_len + 1)\n",
    "    line2 += full_label.ljust(max_len + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cb07e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b563a036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(dataset[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34e323fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b23b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to align labels with tokenized inputs\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_ids != current_word:\n",
    "            # Start of a new word\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX, change it to I-XXX for subsequent tokens\n",
    "            if label % 2 == 1:  # Odd index indicates B-XXX\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dba3ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "aligned_labels = align_labels_with_tokens(labels, word_ids)\n",
    "print(aligned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "197b0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "defa7e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7118e29b134bfdabfb05529be8c04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770d4d3ffe8f4b278655e63c759dc591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825c90bda86c4f9d981494e513648ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5151f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning the model with the trainer API\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c7d5919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6db42bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
      "[-100, 1, 2, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85f80fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c9cfef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[l] for l in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9646fdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': np.float64(1.0),\n",
       "  'recall': np.float64(0.5),\n",
       "  'f1': np.float64(0.6666666666666666),\n",
       "  'number': np.int64(2)},\n",
       " 'ORG': {'precision': np.float64(1.0),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1': np.float64(1.0),\n",
       "  'number': np.int64(1)},\n",
       " 'overall_precision': np.float64(1.0),\n",
       " 'overall_recall': np.float64(0.6666666666666666),\n",
       " 'overall_f1': np.float64(0.8),\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36f3f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2dd75c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7711d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set up the model\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b28b12e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "870b2eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tensor-polinomics/tokenizer_python_52k/commit/02c25425bb0e3d7b448581ad4472a23b76373de1', commit_message='Upload tokenizer', commit_description='', oid='02c25425bb0e3d7b448581ad4472a23b76373de1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tensor-polinomics/tokenizer_python_52k', endpoint='https://huggingface.co', repo_type='model', repo_id='tensor-polinomics/tokenizer_python_52k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Authenticate Hugging Face Hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    "    \"tensor-polinomics/tokenizer_python_52k\",\n",
    "    token=os.getenv(\"HF_TOKEN_WRITE\")  # Bypasses all cached credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e75e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/mnt/ebs1/yluo/projects/learning/learning_misc/hf_llm/notebooks/wandb/offline-run-20251214_005703-ewba20qn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ebs1/yluo/projects/learning/learning_misc/hf_llm/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 2:55:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.065672</td>\n",
       "      <td>0.922775</td>\n",
       "      <td>0.929481</td>\n",
       "      <td>0.926116</td>\n",
       "      <td>0.982310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.067842</td>\n",
       "      <td>0.940368</td>\n",
       "      <td>0.939440</td>\n",
       "      <td>0.939904</td>\n",
       "      <td>0.985077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.066005</td>\n",
       "      <td>0.941729</td>\n",
       "      <td>0.943926</td>\n",
       "      <td>0.942826</td>\n",
       "      <td>0.985798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ebs1/yluo/projects/learning/learning_misc/hf_llm/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/mnt/ebs1/yluo/projects/learning/learning_misc/hf_llm/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.0723088124346896, metrics={'train_runtime': 10521.092, 'train_samples_per_second': 4.004, 'train_steps_per_second': 0.501, 'total_flos': 920771584279074.0, 'train_loss': 0.0723088124346896, 'epoch': 3.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../data/bert-finetuned-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7188a230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/models/bert-finetuned-ner/tokenizer_config.json',\n",
       " '../data/models/bert-finetuned-ner/special_tokens_map.json',\n",
       " '../data/models/bert-finetuned-ner/vocab.txt',\n",
       " '../data/models/bert-finetuned-ner/added_tokens.json',\n",
       " '../data/models/bert-finetuned-ner/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer to disk\n",
    "trainer.save_model(\"../data/models/bert-finetuned-ner\")\n",
    "tokenizer.save_pretrained(\"../data/models/bert-finetuned-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0fc1fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '689fea6e1e6dcf030e6e0b99', 'name': 'tensor-polinomics', 'fullname': 'Emma Luo', 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gKiHqz6Pl_ACJRxiiU_Dk.png', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'hf_llm', 'role': 'fineGrained', 'createdAt': '2025-12-06T16:39:22.210Z', 'fineGrained': {'canReadGatedRepos': True, 'global': [], 'scoped': [{'entity': {'_id': '689fea6e1e6dcf030e6e0b99', 'type': 'user', 'name': 'tensor-polinomics'}, 'permissions': ['repo.content.read', 'inference.serverless.write']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "info = whoami()\n",
    "print(info)  # Check if 'auth' shows write access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bec9c213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bbd6d2da5544ab8e3bde199beae7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bfa0bb3bd841819cc0fdfe45780e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tensor-polinomics/bert-finetuned-ner/commit/d7ccd15715bdd55b6f2a357e1de8eabd8eb33adb', commit_message='Training complete', commit_description='', oid='d7ccd15715bdd55b6f2a357e1de8eabd8eb33adb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tensor-polinomics/bert-finetuned-ner', endpoint='https://huggingface.co', repo_type='model', repo_id='tensor-polinomics/bert-finetuned-ner'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the most recent version to the Hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "trainer.push_to_hub(\n",
    "    commit_message=\"Training complete\",\n",
    "    token=os.getenv(\"HF_TOKEN_WRITE\")  # Bypasses all cached credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7032fda",
   "metadata": {},
   "source": [
    "# Section 2. A Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataloaders manually (instead of using Trainer)\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator, # define earlier\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator, # define earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstantiate the model\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model_custom = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model_custom.parameters(), lr=2e-5)\n",
    "\n",
    "# Add Accelerator for easy device management\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model_custom, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model_custom, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "from transformers import get_scheduler\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Define a helper function to facilitate evaluation\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    \n",
    "    # Remove ignored index (special tokens) and covert to labels\n",
    "    true_labels = [\n",
    "        [label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_predictions, true_labels\n",
    "\n",
    "# Build the training loop:\n",
    "# 1) training itself with train_dataloader\n",
    "# 2) eval with accelerator.pad_across_processes\n",
    "# 3) save and upload with repo_push_to_hub\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        # padding\n",
    "        predictions = accelerator.pad_across_processes(\n",
    "            predictions, dim=1, pad_index=-100\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        \n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        \n",
    "        true_predictions, true_labels = postprocess(\n",
    "            predictions_gathered, labels_gathered\n",
    "        )\n",
    "        metric.add_batch(\n",
    "            predictions=true_predictions,\n",
    "            references=true_labels,\n",
    "        )\n",
    "        \n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"] \n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "# Save and upload\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "\n",
    "# Create repo\n",
    "repo_id = create_repo(model_name, token=token, exist_ok=True).repo_id\n",
    "print(f\"Repo created: {repo_id}\")\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(\n",
    "    \"../data/models/bert-finetuned-ner-accelerate\",\n",
    "    save_function=accelerator.save,\n",
    ")\n",
    "if accelerator.is_main_process:\n",
    "    tokenizer.save_pretrained(\"../data/models/bert-finetuned-ner-accelerate\")\n",
    "    model.push_to_hub(model_name, token=token)\n",
    "    tokenizer.push_to_hub(model_name, token=token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
