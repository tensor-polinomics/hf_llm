{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e70123",
   "metadata": {},
   "source": [
    "## Traing a causal language model for Python data science code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e08291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    return any(keyword in string for keyword in keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74893446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"polars\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "print(any_keyword_in_string(example_1, filters))  # Should print False\n",
    "print(any_keyword_in_string(example_2, filters))  # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8844ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even the sample data is rather large, so we will filter it down using streaming\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "ds_train_stream = load_dataset(\n",
    "    \"huggingface-course/codeparrot-ds-train\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ").take(20_000)\n",
    "ds_val_stream = load_dataset(\n",
    "    \"huggingface-course/codeparrot-ds-valid\",\n",
    "    split=\"validation\",\n",
    "    streaming=True,\n",
    ").take(5_000)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_list(list(ds_train_stream)),\n",
    "    \"validation\": Dataset.from_list(list(ds_val_stream)),\n",
    "})\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822647f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'kmike/scikit-learn',\n",
       " 'path': 'sklearn/utils/__init__.py',\n",
       " 'copies': '3',\n",
       " 'size': '10094',\n",
       " 'content': '\"\"\"\\nThe :mod:`sklearn.utils` module includes various utilites.\\n\"\"\"\\n\\nfrom collections import Sequence\\n\\nimport numpy as np\\nfrom scipy.sparse import issparse\\nimport warnings\\n\\nfrom .murmurhash import murmurhash3_32\\nfrom .validation import (as_float_array, check_arrays, safe_asarray,\\n                         assert_all_finite, array2d, atleast2d_or_csc,\\n                         atleast2d_or_csr, warn_if_not_float,\\n                         check_random_state)\\nfrom .class_weight import compute_class_weight\\n\\n__all__ = [\"murmurhash3_32\", \"as_float_array\", \"check_arrays\", \"safe_asarray\",\\n           \"assert_all_finite\", \"array2d\", \"atleast2d_or_csc\",\\n           \"atleast2d_or_csr\", \"warn_if_not_float\", \"check_random_state\",\\n           \"compute_class_weight\"]\\n\\n# Make sure that DeprecationWarning get printed\\nwarnings.simplefilter(\"always\", DeprecationWarning)\\n\\n\\nclass deprecated(object):\\n    \"\"\"Decorator to mark a function or class as deprecated.\\n\\n    Issue a warning when the function is called/the class is instantiated and\\n    adds a warning to the docstring.\\n\\n    The optional extra argument will be appended to the deprecation message\\n    and the docstring. Note: to use this with the default value for extra, put\\n    in an empty of parentheses:\\n\\n    >>> from sklearn.utils import deprecated\\n    >>> deprecated() # doctest: +ELLIPSIS\\n    <sklearn.utils.deprecated object at ...>\\n\\n    >>> @deprecated()\\n    ... def some_function(): pass\\n    \"\"\"\\n\\n    # Adapted from http://wiki.python.org/moin/PythonDecoratorLibrary,\\n    # but with many changes.\\n\\n    def __init__(self, extra=\\'\\'):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        extra: string\\n          to be added to the deprecation messages\\n\\n        \"\"\"\\n        self.extra = extra\\n\\n    def __call__(self, obj):\\n        if isinstance(obj, type):\\n            return self._decorate_class(obj)\\n        else:\\n            return self._decorate_fun(obj)\\n\\n    def _decorate_class(self, cls):\\n        msg = \"Class %s is deprecated\" % cls.__name__\\n        if self.extra:\\n            msg += \"; %s\" % self.extra\\n\\n        # FIXME: we should probably reset __new__ for full generality\\n        init = cls.__init__\\n\\n        def wrapped(*args, **kwargs):\\n            warnings.warn(msg, category=DeprecationWarning)\\n            return init(*args, **kwargs)\\n        cls.__init__ = wrapped\\n\\n        wrapped.__name__ = \\'__init__\\'\\n        wrapped.__doc__ = self._update_doc(init.__doc__)\\n        wrapped.deprecated_original = init\\n\\n        return cls\\n\\n    def _decorate_fun(self, fun):\\n        \"\"\"Decorate function fun\"\"\"\\n\\n        msg = \"Function %s is deprecated\" % fun.__name__\\n        if self.extra:\\n            msg += \"; %s\" % self.extra\\n\\n        def wrapped(*args, **kwargs):\\n            warnings.warn(msg, category=DeprecationWarning)\\n            return fun(*args, **kwargs)\\n\\n        wrapped.__name__ = fun.__name__\\n        wrapped.__dict__ = fun.__dict__\\n        wrapped.__doc__ = self._update_doc(fun.__doc__)\\n\\n        return wrapped\\n\\n    def _update_doc(self, olddoc):\\n        newdoc = \"DEPRECATED\"\\n        if self.extra:\\n            newdoc = \"%s: %s\" % (newdoc, self.extra)\\n        if olddoc:\\n            newdoc = \"%s\\\\n\\\\n%s\" % (newdoc, olddoc)\\n        return newdoc\\n\\n\\ndef safe_mask(X, mask):\\n    \"\"\"Return a mask which is safe to use on X.\\n\\n    Parameters\\n    ----------\\n    X : {array-like, sparse matrix}\\n        Data on which to apply mask.\\n\\n    mask: array\\n        Mask to be used on X.\\n\\n    Returns\\n    -------\\n        mask\\n    \"\"\"\\n    mask = np.asanyarray(mask)\\n    if np.issubdtype(mask.dtype, np.int):\\n        return mask\\n\\n    if hasattr(X, \"toarray\"):\\n        ind = np.arange(mask.shape[0])\\n        mask = ind[mask]\\n    return mask\\n\\n\\ndef resample(*arrays, **options):\\n    \"\"\"Resample arrays or sparse matrices in a consistent way\\n\\n    The default strategy implements one step of the bootstrapping\\n    procedure.\\n\\n    Parameters\\n    ----------\\n    `*arrays` : sequence of arrays or scipy.sparse matrices with same shape[0]\\n\\n    replace : boolean, True by default\\n        Implements resampling with replacement. If False, this will implement\\n        (sliced) random permutations.\\n\\n    n_samples : int, None by default\\n        Number of samples to generate. If left to None this is\\n        automatically set to the first dimension of the arrays.\\n\\n    random_state : int or RandomState instance\\n        Control the shuffling for reproducible behavior.\\n\\n    Returns\\n    -------\\n    Sequence of resampled views of the collections. The original arrays are\\n    not impacted.\\n\\n    Examples\\n    --------\\n    It is possible to mix sparse and dense arrays in the same run::\\n\\n      >>> X = [[1., 0.], [2., 1.], [0., 0.]]\\n      >>> y = np.array([0, 1, 2])\\n\\n      >>> from scipy.sparse import coo_matrix\\n      >>> X_sparse = coo_matrix(X)\\n\\n      >>> from sklearn.utils import resample\\n      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\\n      >>> X\\n      array([[ 1.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\\n      <3x2 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\n          with 4 stored elements in Compressed Sparse Row format>\\n\\n      >>> X_sparse.toarray()\\n      array([[ 1.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> y\\n      array([0, 1, 0])\\n\\n      >>> resample(y, n_samples=2, random_state=0)\\n      array([0, 1])\\n\\n\\n    See also\\n    --------\\n    :class:`sklearn.cross_validation.Bootstrap`\\n    :func:`sklearn.utils.shuffle`\\n    \"\"\"\\n    random_state = check_random_state(options.pop(\\'random_state\\', None))\\n    replace = options.pop(\\'replace\\', True)\\n    max_n_samples = options.pop(\\'n_samples\\', None)\\n    if options:\\n        raise ValueError(\"Unexpected kw arguments: %r\" % options.keys())\\n\\n    if len(arrays) == 0:\\n        return None\\n\\n    first = arrays[0]\\n    n_samples = first.shape[0] if hasattr(first, \\'shape\\') else len(first)\\n\\n    if max_n_samples is None:\\n        max_n_samples = n_samples\\n\\n    if max_n_samples > n_samples:\\n        raise ValueError(\"Cannot sample %d out of arrays with dim %d\" % (\\n            max_n_samples, n_samples))\\n\\n    arrays = check_arrays(*arrays, sparse_format=\\'csr\\')\\n\\n    if replace:\\n        indices = random_state.randint(0, n_samples, size=(max_n_samples,))\\n    else:\\n        indices = np.arange(n_samples)\\n        random_state.shuffle(indices)\\n        indices = indices[:max_n_samples]\\n\\n    resampled_arrays = []\\n\\n    for array in arrays:\\n        array = array[indices]\\n        resampled_arrays.append(array)\\n\\n    if len(resampled_arrays) == 1:\\n        # syntactic sugar for the unit argument case\\n        return resampled_arrays[0]\\n    else:\\n        return resampled_arrays\\n\\n\\ndef shuffle(*arrays, **options):\\n    \"\"\"Shuffle arrays or sparse matrices in a consistent way\\n\\n    This is a convenience alias to ``resample(*arrays, replace=False)`` to do\\n    random permutations of the collections.\\n\\n    Parameters\\n    ----------\\n    `*arrays` : sequence of arrays or scipy.sparse matrices with same shape[0]\\n\\n    random_state : int or RandomState instance\\n        Control the shuffling for reproducible behavior.\\n\\n    n_samples : int, None by default\\n        Number of samples to generate. If left to None this is\\n        automatically set to the first dimension of the arrays.\\n\\n    Returns\\n    -------\\n    Sequence of shuffled views of the collections. The original arrays are\\n    not impacted.\\n\\n    Examples\\n    --------\\n    It is possible to mix sparse and dense arrays in the same run::\\n\\n      >>> X = [[1., 0.], [2., 1.], [0., 0.]]\\n      >>> y = np.array([0, 1, 2])\\n\\n      >>> from scipy.sparse import coo_matrix\\n      >>> X_sparse = coo_matrix(X)\\n\\n      >>> from sklearn.utils import shuffle\\n      >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\\n      >>> X\\n      array([[ 0.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\\n      <3x2 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\n          with 3 stored elements in Compressed Sparse Row format>\\n\\n      >>> X_sparse.toarray()\\n      array([[ 0.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> y\\n      array([2, 1, 0])\\n\\n      >>> shuffle(y, n_samples=2, random_state=0)\\n      array([0, 1])\\n\\n    See also\\n    --------\\n    :func:`sklearn.utils.resample`\\n    \"\"\"\\n    options[\\'replace\\'] = False\\n    return resample(*arrays, **options)\\n\\n\\ndef safe_sqr(X, copy=True):\\n    \"\"\"Element wise squaring of array-likes and sparse matrices.\\n\\n    Parameters\\n    ----------\\n    X : array like, matrix, sparse matrix\\n\\n    Returns\\n    -------\\n    X ** 2 : element wise square\\n    \"\"\"\\n    X = safe_asarray(X)\\n    if issparse(X):\\n        if copy:\\n            X = X.copy()\\n        X.data **= 2\\n    else:\\n        if copy:\\n            X = X ** 2\\n        else:\\n            X **= 2\\n    return X\\n\\n\\ndef gen_even_slices(n, n_packs):\\n    \"\"\"Generator to create n_packs slices going up to n.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.utils import gen_even_slices\\n    >>> list(gen_even_slices(10, 1))\\n    [slice(0, 10, None)]\\n    >>> list(gen_even_slices(10, 10))                     #doctest: +ELLIPSIS\\n    [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]\\n    >>> list(gen_even_slices(10, 5))                      #doctest: +ELLIPSIS\\n    [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]\\n    >>> list(gen_even_slices(10, 3))\\n    [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]\\n    \"\"\"\\n    start = 0\\n    for pack_num in range(n_packs):\\n        this_n = n // n_packs\\n        if pack_num < n % n_packs:\\n            this_n += 1\\n        if this_n > 0:\\n            end = start + this_n\\n            yield slice(start, end, None)\\n            start = end\\n\\n\\ndef tosequence(x):\\n    \"\"\"Cast iterable x to a Sequence, avoiding a copy if possible.\"\"\"\\n    if isinstance(x, np.ndarray):\\n        return np.asarray(x)\\n    elif isinstance(x, Sequence):\\n        return x\\n    else:\\n        return list(x)\\n\\n\\nclass ConvergenceWarning(Warning):\\n    \"Custom warning to capture convergence problems\"\\n',\n",
       " 'license': 'bsd-3-clause'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few examples\n",
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44308de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: kmike/scikit-learn...\n",
      "\n",
      "PATH: sklearn/utils/__init__.py...\n",
      "\n",
      "COPIES: 3...\n",
      "\n",
      "SIZE: 10094...\n",
      "\n",
      "CONTENT: \"\"\"\n",
      "The :mod:`sklearn.utils` module includes various utilites.\n",
      "\"\"\"\n",
      "\n",
      "from collections import Sequence\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import issparse\n",
      "import warnings\n",
      "\n",
      "from .murmurhash import murm...\n",
      "\n",
      "LICENSE: bsd-3-clause...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf1e58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c92dd97b5a4ca09f714857fad8cd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab5b64f70a64962b875077698b76b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce5f1543b6f47ea9acf3cd367fc8e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6915476ced4174a2646a23f6dfb17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e307ad5038504704bad031d54395a474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs lengths: 34\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Cut contents into chunks\n",
    "from transformers import AutoTokenizer\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][\"content\"][:2],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs lengths: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {(outputs['overflow_to_sample_mapping'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7124b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_IDS: [[280, 173, 2096, 310, 2025, 749, 24661, 14, 1377, 64, 1340, 6376, 8258, 4705, 2646, 14, 173, 280, 173, 173, 973, 4962, 978, 7187, 173, 173, 2745, 1601, 442, 635, 173, 973, 4985, 14, 6322, 978, 29638, 173, 2745, 3758, 173, 173, 973, 1428, 77, 26853, 250, 1817, 978, 288, 26853, 250, 1817, 19, 63, 1551, 173, 973, 1428, 6436, 978, 308, 352, 63, 1345, 63, 783, 12, 951, 63, 7194, 12, 4724, 63, 5293, 12, 373, 1387, 63, 536, 63, 7544, 12, 960, 18, 68, 12, 35001, 18, 68, 63, 228, 63, 22229, 12, 373, 35001, 18, 68, 63, 228, 63, 9422, 12, 6792, 63, 804, 63, 854, 63, 1345, 12, 373, 951, 63, 2437, 63, 977, 9, 173, 973, 1428, 692, 63, 2077, 978, 3115, 63], [692, 63, 2077, 173, 173, 612, 536, 612, 233, 2558, 77, 26853, 250, 1817, 19, 63, 1551, 485, 333, 352, 63, 1345, 63, 783, 485, 333, 1207, 63, 7194, 485, 333, 4048, 63, 5293, 485, 1366, 333, 5224, 63, 536, 63, 7544, 485, 333, 783, 18, 68, 485, 333, 11198, 18, 68, 63, 228, 63, 22229, 485, 1366, 333, 11198, 18, 68, 63, 228, 63, 9422, 485, 333, 3092, 63, 804, 63, 854, 63, 1345, 485, 333, 1207, 63, 2437, 63, 977, 485, 1366, 333, 4180, 63, 692, 63, 2077, 778, 173, 173, 3, 3596, 2926, 542, 12925, 549, 7784, 173, 7150, 14, 21848, 439, 10353, 485, 12925, 9, 4391, 173, 692, 5580, 8, 1023, 274, 232, 290, 12392, 292, 2391, 231, 753, 385, 1149, 442, 5580, 14], [312, 16376, 231, 2671, 1184, 256, 753, 300, 2072, 15, 1941, 1149, 300, 14200, 350, 232, 7078, 231, 2671, 292, 256, 5896, 14, 312, 507, 1154, 1480, 1041, 664, 371, 8415, 292, 256, 19475, 952, 232, 350, 256, 5896, 14, 2321, 26, 292, 811, 551, 461, 256, 713, 448, 296, 1480, 12, 4124, 232, 253, 309, 2039, 311, 26347, 26, 312, 705, 497, 15673, 14, 1377, 978, 5580, 232, 705, 5580, 323, 294, 7844, 26, 382, 18526, 232, 659, 24661, 14, 1377, 14, 11948, 624, 815, 44206, 312, 705, 1111, 11948, 323, 232, 1329, 509, 1791, 63, 1619, 794, 989, 232, 290, 312, 294, 25923, 497, 2297, 1193, 5184, 14, 2580, 14, 2129, 15, 607, 219, 15, 6545, 12392, 12532, 12, 232, 294, 1314, 461, 3729, 3099, 14], [312, 509, 1289, 1863, 4223, 248, 12, 1480, 7967, 222, 290, 222, 1174, 222, 1195, 222, 1480, 26, 671, 880, 292, 371, 2621, 292, 256, 19475, 3039, 298, 290, 222, 272, 14, 2196, 233, 1480, 312, 509, 1289, 1096, 4223, 248, 12, 1300, 274, 222, 264, 735, 8, 886, 12, 630, 274, 241, 302, 272, 337, 22733, 63, 692, 8, 886, 9, 222, 425, 26, 241, 302, 272, 337, 22733, 63, 4072, 8, 886, 9, 312, 509, 409, 22733, 63, 692, 8, 248, 12, 1429, 274, 222, 1347, 233, 333, 2425, 446, 83, 300, 5580, 2, 446, 1429, 814, 324, 612, 222, 264, 272, 14, 2196, 26, 241, 1347, 793, 20357, 446, 83, 2, 446, 272, 14, 2196, 298, 294, 7469, 26, 649, 1037, 7125, 4096, 1289, 1168], [612, 296, 2026, 1251, 4238, 222, 2884, 233, 1429, 814, 1863, 612, 298, 509, 5150, 1614, 426, 12, 655, 527, 274, 241, 3758, 14, 3092, 8, 1324, 12, 4176, 29, 24112, 9, 241, 302, 2884, 1614, 426, 12, 655, 527, 9, 222, 1429, 814, 1863, 612, 233, 5150, 298, 5150, 814, 324, 612, 233, 4488, 1863, 14055, 222, 5150, 814, 1276, 612, 233, 272, 337, 716, 63, 1276, 8, 1863, 814, 1276, 3485, 222, 5150, 14, 11948, 63, 4884, 233, 2884, 298, 302, 1429, 312, 509, 409, 22733, 63, 4072, 8, 248, 12, 5715, 274, 222, 290, 33531, 753, 5715, 280, 298, 1347, 233, 333, 4680, 446, 83, 300, 5580, 2, 446, 5715, 814, 324, 612, 222, 264, 272, 14, 2196, 26, 241, 1347, 793, 20357, 446, 83], [2, 446, 272, 14, 2196, 298, 509, 5150, 1614, 426, 12, 655, 527, 274, 241, 3758, 14, 3092, 8, 1324, 12, 4176, 29, 24112, 9, 241, 302, 5715, 1614, 426, 12, 655, 527, 9, 298, 5150, 814, 324, 612, 233, 5715, 814, 324, 612, 222, 5150, 814, 645, 612, 233, 5715, 814, 645, 612, 222, 5150, 814, 1276, 612, 233, 272, 337, 716, 63, 1276, 8, 4072, 814, 1276, 3485, 298, 302, 5150, 312, 509, 409, 716, 63, 1276, 8, 248, 12, 7960, 44863, 274, 222, 643, 1276, 233, 333, 25354, 2, 222, 264, 272, 14, 2196, 26, 241, 643, 1276, 233, 2343, 83, 26, 446, 83, 2, 446, 308, 1168, 1276, 12, 272, 14, 2196, 9, 222, 264, 7960, 44863, 26, 241, 643, 1276, 233, 2343, 83], [60, 78, 60, 78, 5, 83, 2, 446, 308, 1168, 1276, 12, 7960, 44863, 9, 222, 302, 643, 1276, 4391, 173, 295, 4724, 63, 1813, 8, 56, 12, 2185, 274, 232, 290, 1798, 231, 2185, 947, 300, 4724, 292, 811, 517, 1247, 14, 312, 1174, 232, 1195, 232, 1247, 310, 420, 783, 13, 1696, 12, 5721, 1848, 93, 222, 1802, 517, 947, 292, 2937, 2185, 14, 312, 2185, 26, 960, 222, 16334, 292, 371, 958, 517, 1247, 14, 312, 734, 232, 1513, 222, 2185, 232, 290, 232, 2185, 233, 635, 14, 18317, 8, 1813, 9, 232, 264, 635, 14, 23198, 8, 1813, 14, 2046, 12, 635, 14, 384, 274, 222, 302, 2185, 312, 264, 1781, 8, 56, 12, 333, 22816, 1979, 222, 2329, 233, 635, 14, 4419, 8], [1813, 14, 1028, 59, 16, 535, 222, 2185, 233, 2329, 59, 1813, 61, 232, 302, 2185, 4391, 173, 295, 11494, 1614, 7194, 12, 655, 1440, 274, 232, 290, 1407, 642, 3796, 385, 5721, 8542, 253, 231, 7638, 3630, 312, 507, 713, 8387, 14635, 981, 2232, 311, 256, 38642, 232, 12482, 14, 312, 1174, 232, 1195, 232, 40038, 7194, 64, 310, 1969, 311, 3796, 385, 4985, 14, 6322, 8542, 461, 1496, 1593, 59, 16, 61, 312, 2330, 310, 2861, 12, 623, 585, 713, 222, 17718, 19316, 461, 7436, 14, 647, 693, 12, 551, 664, 3677, 222, 308, 23418, 9, 2280, 19740, 14, 312, 252, 63, 2461, 310, 698, 12, 377, 585, 713, 222, 3482, 311, 2751, 292, 2665, 14, 647, 2541, 292, 377, 551, 300, 222, 4322, 562, 292], [256, 1165, 3300, 311, 256, 3796, 14, 312, 2280, 63, 977, 310, 698, 385, 30288, 942, 222, 15103, 256, 48887, 296, 49400, 6399, 14, 312, 734, 232, 1513, 232, 7187, 311, 18407, 10999, 311, 256, 4962, 14, 507, 2405, 3796, 602, 232, 339, 9622, 301, 14, 312, 3142, 232, 1009, 232, 2296, 300, 2738, 292, 9413, 5721, 350, 12461, 3796, 253, 256, 1496, 1052, 1125, 2914, 705, 1247, 233, 4326, 17, 1995, 443, 9449, 404, 18, 1995, 396, 9449, 404, 16, 1995, 443, 29689, 558, 705, 522, 233, 635, 14, 783, 929, 16, 12, 396, 12, 554, 535, 2914, 705, 497, 4985, 14, 6322, 978, 23579, 63, 2271, 558, 705, 1247, 63, 6322, 233, 23579, 63, 2271, 8, 56, 9, 2914, 705, 497, 15673, 14, 1377, 978, 11494], [558, 705, 1247, 12, 1247, 63, 6322, 12, 522, 233, 11494, 8, 56, 12, 1247, 63, 6322, 12, 522, 12, 2280, 63, 977, 29, 16, 9, 558, 705, 1247, 558, 960, 5773, 396, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 1247, 63, 6322, 7700, 294, 7844, 26, 382, 18526, 382, 23984, 63, 22062, 558, 659, 19, 88, 18, 5721, 1848, 311, 630, 4755, 1653, 269, 2786, 14, 1345, 1508, 7, 5924, 880, 461, 1163, 3394, 2347, 253, 2705, 3791, 16662, 11072, 1115, 30, 2914, 705, 1247, 63, 6322, 14, 22816, 323, 558, 960, 5773, 396, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705], [522, 558, 960, 929, 16, 12, 396, 12, 443, 535, 2914, 705, 11494, 8, 89, 12, 252, 63, 2461, 29, 18, 12, 2280, 63, 977, 29, 16, 9, 558, 960, 929, 16, 12, 396, 535, 4601, 2206, 2091, 232, 1009, 232, 310, 692, 749, 24661, 14, 6571, 63, 6436, 14, 35270, 64, 232, 310, 1117, 749, 24661, 14, 1377, 14, 11907, 64, 232, 290, 232, 2280, 63, 977, 233, 951, 63, 2437, 63, 977, 8, 1440, 14, 1331, 359, 2437, 63, 977, 340, 377, 353, 232, 2330, 233, 1401, 14, 1331, 359, 1510, 340, 623, 9, 232, 901, 63, 78, 63, 2461, 233, 1401, 14, 1331, 359, 78, 63, 2461, 340, 377, 9, 232, 264, 1401, 26, 222, 510, 911, 439, 8926, 3718, 1445, 26, 446, 82, 2], [446, 1401, 14, 1077, 1002, 312, 264, 553, 8, 7194, 9, 451, 443, 26, 222, 302, 377, 312, 1165, 233, 3796, 59, 16, 61, 232, 252, 63, 2461, 233, 1165, 14, 1028, 59, 16, 61, 264, 1781, 8, 2387, 12, 269, 1028, 397, 425, 553, 8, 2387, 9, 312, 264, 901, 63, 78, 63, 2461, 300, 377, 26, 222, 901, 63, 78, 63, 2461, 233, 252, 63, 2461, 312, 264, 901, 63, 78, 63, 2461, 751, 252, 63, 2461, 26, 222, 510, 911, 439, 4453, 1945, 446, 68, 555, 311, 3796, 461, 1805, 446, 68, 2, 446, 308, 241, 901, 63, 78, 63, 2461, 12, 252, 63, 2461, 353, 312, 3796, 233, 951, 63, 7194, 1614, 7194, 12, 5721, 63, 514, 567, 9422, 397, 312, 264, 2330, 26], [222, 2784, 233, 2280, 63, 977, 14, 8427, 8, 16, 12, 252, 63, 2461, 12, 1208, 2471, 852, 63, 78, 63, 2461, 5154, 232, 425, 26, 222, 2784, 233, 635, 14, 4419, 8, 78, 63, 2461, 9, 222, 2280, 63, 977, 14, 11907, 8, 2662, 9, 222, 2784, 233, 2784, 1018, 852, 63, 78, 63, 2461, 61, 312, 18407, 63, 7194, 233, 787, 312, 296, 960, 253, 3796, 26, 222, 960, 233, 960, 59, 2662, 61, 222, 18407, 63, 7194, 14, 576, 8, 783, 9, 312, 264, 553, 8, 26911, 63, 7194, 9, 451, 396, 26, 222, 294, 30455, 259, 35516, 296, 256, 2593, 1041, 1539, 222, 302, 18407, 63, 7194, 59, 16, 61, 232, 425, 26, 222, 302, 18407, 63, 7194, 4391, 173, 295, 11226, 1614, 7194], [12, 655, 1440, 274, 232, 290, 2885, 7313, 3796, 385, 5721, 8542, 253, 231, 7638, 3630, 312, 841, 300, 231, 11295, 3980, 292, 581, 11748, 1614, 7194, 12, 2330, 29, 722, 6414, 292, 764, 232, 2280, 19740, 311, 256, 4962, 14, 312, 1174, 232, 1195, 232, 40038, 7194, 64, 310, 1969, 311, 3796, 385, 4985, 14, 6322, 8542, 461, 1496, 1593, 59, 16, 61, 312, 2280, 63, 977, 310, 698, 385, 30288, 942, 222, 15103, 256, 48887, 296, 49400, 6399, 14, 312, 252, 63, 2461, 310, 698, 12, 377, 585, 713, 222, 3482, 311, 2751, 292, 2665, 14, 647, 2541, 292, 377, 551, 300, 222, 4322, 562, 292, 256, 1165, 3300, 311, 256, 3796, 14, 312, 734, 232, 1513, 232, 7187, 311, 31575, 10999, 311, 256, 4962, 14], [507, 2405, 3796, 602, 232, 339, 9622, 301, 14, 312, 3142, 232, 1009, 232, 2296, 300, 2738, 292, 9413, 5721, 350, 12461, 3796, 253, 256, 1496, 1052, 1125, 2914, 705, 1247, 233, 4326, 17, 1995, 443, 9449, 404, 18, 1995, 396, 9449, 404, 16, 1995, 443, 29689, 558, 705, 522, 233, 635, 14, 783, 929, 16, 12, 396, 12, 554, 535, 2914, 705, 497, 4985, 14, 6322, 978, 23579, 63, 2271, 558, 705, 1247, 63, 6322, 233, 23579, 63, 2271, 8, 56, 9, 2914, 705, 497, 15673, 14, 1377, 978, 11226, 558, 705, 1247, 12, 1247, 63, 6322, 12, 522, 233, 11226, 8, 56, 12, 1247, 63, 6322, 12, 522, 12, 2280, 63, 977, 29, 16, 9, 558, 705, 1247, 558, 960, 5773, 443, 1995, 179, 443, 9449], [1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 1247, 63, 6322, 7700, 294, 7844, 26, 382, 18526, 382, 23984, 63, 22062, 558, 659, 19, 88, 18, 5721, 1848, 311, 630, 4755, 1653, 269, 2786, 14, 1345, 1508, 7, 5924, 880, 461, 869, 3394, 2347, 253, 2705, 3791, 16662, 11072, 1115, 30, 2914, 705, 1247, 63, 6322, 14, 22816, 323, 558, 960, 5773, 443, 1995, 179, 443, 9449, 1560, 404, 554, 1995, 179, 396, 9449, 1560, 404, 396, 1995, 179, 443, 26922, 2914, 705, 522, 558, 960, 929, 18, 12, 396, 12, 443, 535, 2914, 705, 11226, 8, 89, 12, 252, 63, 2461, 29, 18, 12, 2280, 63, 977, 29, 16, 9, 558, 960, 929, 16, 12, 396, 535, 312, 2206], [2091, 232, 1009, 232, 310, 1117, 749, 24661, 14, 1377, 14, 11748, 64, 232, 290, 232, 1401, 357, 1510, 358, 233, 693, 232, 302, 11494, 1614, 7194, 12, 655, 1440, 9, 4391, 173, 295, 4724, 63, 25669, 8, 56, 12, 1875, 29, 678, 274, 232, 290, 1611, 34080, 38030, 8658, 311, 960, 13, 24183, 350, 5721, 8542, 14, 312, 1174, 232, 1195, 232, 1247, 310, 960, 2154, 12, 1848, 12, 5721, 1848, 312, 734, 232, 1513, 232, 1247, 655, 554, 310, 1166, 34080, 7912, 232, 290, 232, 1247, 233, 4724, 63, 5293, 8, 56, 9, 232, 264, 29638, 8, 56, 274, 222, 264, 1875, 26, 241, 1247, 233, 1247, 14, 1556, 323, 222, 1247, 14, 441, 655, 29, 554, 232, 425, 26, 222, 264, 1875, 26, 241, 1247], [233, 1247, 655, 554, 222, 425, 26, 241, 1247, 655, 29, 554, 232, 302, 1247, 4391, 173, 295, 2055, 63, 13798, 63, 9352, 8, 78, 12, 252, 63, 34671, 274, 232, 290, 8408, 292, 969, 252, 63, 34671, 9301, 7495, 1129, 292, 252, 14, 312, 3142, 232, 1009, 232, 705, 497, 15673, 14, 1377, 978, 2055, 63, 13798, 63, 9352, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 396, 353, 232, 404, 3741, 8, 16, 12, 2009, 12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 2009, 353, 9774, 294, 23143, 26, 382, 18526, 232, 404, 3741, 8, 16, 12, 396, 12, 377, 491, 3905, 8, 17, 12, 554, 12, 377, 491, 9640, 3905, 8, 25, 12, 2009], [12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 1462, 353, 9853, 294, 23143, 26, 382, 18526, 232, 404, 3741, 8, 16, 12, 554, 12, 377, 491, 3905, 8, 18, 12, 1163, 12, 377, 491, 9640, 3905, 8, 24, 12, 2009, 12, 377, 1460, 232, 705, 495, 8, 1634, 63, 13798, 63, 9352, 8, 1328, 12, 869, 353, 232, 404, 3741, 8, 16, 12, 1163, 12, 377, 491, 3905, 8, 20, 12, 2624, 12, 377, 491, 3905, 8, 23, 12, 2009, 12, 377, 1460, 232, 290, 232, 888, 233, 443, 232, 296, 5435, 63, 810, 253, 1004, 8, 78, 63, 34671, 274, 222, 551, 63, 78, 233, 252, 3864, 252, 63, 34671, 222, 264, 5435, 63, 810, 659, 252, 446, 252, 63], [34671, 26, 241, 551, 63, 78, 793, 396, 222, 264, 551, 63, 78, 751, 443, 26, 241, 974, 233, 888, 382, 551, 63, 78, 241, 1483, 3905, 8, 776, 12, 974, 12, 377, 9, 241, 888, 233, 974, 4391, 173, 295, 292, 2823, 8, 88, 274, 232, 290, 27640, 3209, 548, 292, 231, 7187, 12, 29289, 231, 1875, 264, 2738, 881, 232, 264, 735, 8, 88, 12, 635, 14, 2527, 274, 222, 302, 635, 14, 5293, 8, 88, 9, 232, 639, 735, 8, 88, 12, 7187, 274, 222, 302, 548, 232, 425, 26, 222, 302, 495, 8, 88, 9, 4391, 173, 692, 44910, 4491, 8, 4491, 274, 232, 333, 9763, 2671, 292, 8398, 12169, 7316, 2, 173], [280, 173, 26713, 1750, 1036, 173, 1559, 1500, 253, 626, 13, 6078, 3000, 433, 1312, 256, 17079, 39612, 18610, 308, 35, 3110, 9, 173, 26713, 1750, 1036, 173, 173, 2096, 626, 13, 6078, 15133, 300, 6977, 585, 16321, 1187, 2111, 433, 542, 173, 2264, 2085, 6015, 13, 13488, 815, 2597, 9262, 14, 841, 300, 958, 292, 3115, 231, 173, 14190, 1848, 1187, 1086, 4647, 385, 231, 19564, 626, 13, 2773, 350, 2721, 256, 447, 3110, 173, 8038, 7623, 14, 393, 5273, 38564, 831, 11459, 300, 1319, 4498, 292, 2874, 173, 10639, 14, 173, 280, 173, 3, 5764, 1266, 26, 9491, 22171, 704, 87, 7241, 3608, 659, 306, 22171, 14, 10304, 7241, 3608, 32, 9974, 85, 14, 11016, 30, 173, 3, 3708, 1367, 46733, 13, 405, 1241, 2020, 266], [659, 3497, 254, 221, 1241, 14, 6278, 32, 29024, 14, 1026, 30, 173, 3, 3708, 41336, 19414, 918, 831, 659, 238, 334, 402, 221, 14, 2007, 918, 831, 32, 29024, 14, 1026, 30, 173, 3, 3708, 41336, 402, 221, 704, 5872, 11306, 659, 238, 334, 402, 221, 14, 1866, 11306, 32, 219, 30343, 14, 7164, 30, 173, 3, 173, 3, 24384, 26, 37634, 308, 19, 13, 10492, 9, 4391, 173, 2745, 1601, 442, 635, 173, 2745, 4855, 14, 11032, 442, 2564, 173, 173, 973, 44445, 978, 26497, 83, 12, 969, 63, 666, 12, 3096, 63, 973, 63, 7140, 173, 973, 44445, 14, 1425, 978, 14885, 63, 1311, 83, 12, 1095, 63, 1311, 63, 28807, 173, 973, 44445, 14, 7511, 978, 19960, 3661, 482, 173, 973, 44445, 14, 24983], [978, 447, 3110, 173, 973, 44445, 14, 513, 63, 6078, 978, 18140, 13175, 50, 173, 173, 973, 15673, 14, 1323, 17464, 831, 63, 5188, 978, 13885, 3743, 17464, 831, 9099, 173, 973, 15673, 14, 961, 63, 5178, 978, 7894, 225, 3859, 43, 30709, 12, 5665, 63, 307, 63, 2988, 173, 973, 15673, 14, 5763, 978, 1409, 63, 5763, 173, 973, 15673, 14, 30635, 978, 9558, 11247, 173, 173, 5824, 5824, 44032, 173, 3, 1743, 1439, 350, 1095, 433, 173, 1298, 63, 293, 233, 805, 8, 72, 4276, 29, 18, 12, 36773, 29, 19, 9, 179, 294, 13486, 45849, 89, 26, 41332, 6259, 36773, 173, 4052, 233, 396, 173, 9675, 233, 404, 22, 12, 2009, 12, 6710, 61, 173, 1311, 63, 19268, 233, 19960, 3661, 482, 14, 723, 63], [441, 8, 4052, 12, 6518, 9, 173, 1311, 233, 14885, 63, 1311, 83, 929, 628, 63, 1311, 63, 28807, 8, 70, 12, 31447, 29, 678, 9, 296, 242, 253, 2111, 63, 19268, 535, 173, 173, 3, 6331, 1755, 497, 256, 2111, 480, 173, 83, 3946, 233, 2111, 14, 666, 357, 83, 3946, 358, 173, 3481, 12, 409, 233, 3096, 63, 973, 63, 7140, 8, 1311, 12, 1083, 63, 293, 29, 645, 8, 52, 17, 29, 18, 12, 329, 18, 29, 19, 353, 173, 1311, 14, 5328, 63, 1240, 8, 23699, 29, 722, 12, 262, 8428, 29, 678, 12, 13804, 29, 722, 12, 262, 1053, 29, 722, 12, 3543, 567, 5657, 83, 397, 173, 173, 3, 28232, 256, 11459, 1312, 21372, 13, 7632, 4858, 173, 16158, 233, 1409, 63], [5763, 8, 35, 3110, 8, 78, 63, 4263, 29, 20, 12, 1060, 29, 325, 12, 559, 29, 678, 12, 4365, 63, 3281, 29, 722, 491, 370, 13885, 3743, 17464, 831, 9099, 1002, 173, 78, 63, 9779, 233, 1462, 179, 294, 3048, 3729, 26513, 292, 811, 296, 5665, 13, 6436, 173, 7549, 233, 7894, 225, 3859, 43, 30709, 8, 78, 63, 9779, 29, 78, 63, 9779, 12, 11226, 29, 678, 9, 173, 173, 3, 34182, 1874, 3692, 13, 6078, 1439, 173, 22976, 12, 20785, 233, 21345, 3865, 12, 554, 14, 1811, 173, 78, 63, 9952, 233, 2009, 14, 179, 294, 3048, 3729, 3478, 12036, 26, 958, 292, 5294, 2535, 1208, 173, 728, 63, 3946, 233, 1462, 14, 173, 852, 63, 3946, 233, 7088, 14, 173, 78, 63, 13838, 233], [2070, 179, 294, 3048, 3729, 4281, 4906, 292, 811, 173, 173, 3, 28232, 495, 311, 4281, 1004, 4193, 173, 13838, 233, 635, 14, 8042, 8, 728, 63, 3946, 12, 901, 63, 3946, 12, 252, 63, 13838, 9, 179, 294, 15656, 9262, 173, 3946, 63, 6568, 233, 495, 8, 2448, 8, 13838, 3022, 17, 547, 16384, 59, 17, 12123, 179, 294, 1409, 16384, 495, 311, 4193, 173, 173, 3, 31585, 2535, 9677, 497, 256, 901, 4945, 350, 936, 311, 12036, 292, 4227, 12349, 173, 2773, 63, 9005, 233, 308, 78, 63, 9952, 823, 635, 14, 852, 8, 13838, 9, 823, 554, 3577, 173, 18940, 63, 87, 63, 3467, 233, 635, 14, 4419, 8, 22976, 12, 20785, 12, 2535, 63, 9005, 2174, 17, 2056, 173, 78, 63, 7385, 233, 553], [8, 18940, 63, 87, 63, 3467, 9, 173, 173, 3, 19038, 1215, 7291, 173, 239, 233, 9558, 11247, 323, 173, 173, 5824, 5824, 44032, 173, 3, 10082, 2727, 9262, 12, 2937, 11459, 350, 2183, 5410, 173, 173, 3, 2884, 5410, 173, 3946, 63, 5348, 233, 635, 14, 2470, 1139, 78, 63, 13838, 415, 396, 5154, 173, 173, 3, 10082, 2727, 1086, 4281, 1004, 311, 7536, 173, 1000, 4945, 12, 308, 25786, 12, 28583, 9, 253, 2021, 8, 3946, 63, 6568, 274, 312, 294, 31585, 2535, 1208, 2249, 517, 256, 4281, 2806, 958, 232, 283, 63, 708, 233, 252, 63, 9952, 823, 3243, 31673, 382, 22128, 9, 823, 554, 3577, 179, 294, 253, 2565, 312, 294, 8092, 6015, 13, 1685, 1336, 292, 31146, 256, 1133, 9262, 232, 2111, 63], [1250, 233, 2111, 14, 1556, 1239, 1250, 8, 25786, 12, 28583, 12, 252, 63, 3475, 29, 17, 12, 27022, 63, 9838, 567, 8994, 3807, 340, 2935, 2648, 63, 876, 63, 6603, 567, 2952, 397, 312, 294, 6331, 13400, 497, 4911, 433, 12, 9885, 585, 2535, 1208, 232, 13400, 233, 26497, 83, 8, 1311, 63, 1250, 12, 3096, 12, 1083, 63, 293, 12, 26395, 415, 283, 63, 708, 12, 20785, 382, 283, 63, 708, 12, 370, 8412, 29, 722, 12, 11095, 29, 325, 12, 31447, 29, 678, 9, 232, 13400, 14, 4252, 63, 5657, 323, 232, 522, 233, 821, 14, 2770, 63, 2972, 8, 11193, 14, 3481, 1853, 554, 535, 312, 1247, 233, 13400, 14, 322, 63, 441, 323, 312, 294, 6796, 2600, 5410, 1187, 26513, 296, 1086, 4281], [350, 626, 2535, 232, 4945, 63, 5348, 59, 3946, 61, 233, 635, 14, 2257, 8, 6571, 63, 307, 63, 2988, 8, 9411, 29, 16158, 12, 1247, 29, 56, 12, 522, 29, 89, 12, 4899, 16913, 567, 18714, 63, 21311, 340, 7240, 29, 7549, 12, 4899, 252, 63, 3475, 29, 17, 491, 1612, 29, 16, 9, 173, 173, 5824, 5824, 44032, 173, 3, 6959, 4281, 1354, 173, 173, 8436, 14, 2007, 8, 13838, 3022, 17, 547, 4945, 63, 5348, 12, 2246, 29, 1075, 14, 2293, 8, 13838, 2174, 16, 547, 222, 3213, 567, 2952, 340, 19392, 567, 6127, 397, 173, 8436, 14, 14423, 8, 13838, 9, 173, 8436, 14, 8093, 929, 16, 12, 396, 535, 173, 8436, 14, 29385, 8, 563, 8, 11193, 357, 717, 444, 1191, 823, 553], [8, 11193, 491, 1639, 567, 75, 340, 15042, 46686, 241, 1215, 567, 38038, 1764, 397, 173, 8436, 14, 6405, 323, 173, 8436, 14, 6996, 359, 17017, 308, 12533, 6546, 173, 8436, 14, 6988, 359, 1559, 1500, 438, 2829, 397, 173, 8436, 14, 1687, 359, 17017, 46902, 438, 2829, 397, 173, 173, 5824, 5824, 44032, 173, 3, 10082, 2727, 9262, 350, 626, 12, 2937, 11459, 350, 2183, 5410, 173, 173, 3, 2884, 5410, 173, 2932, 63, 5348, 233, 635, 14, 2470, 1139, 78, 63, 13838, 415, 396, 12, 252, 63, 7385, 353, 173, 173, 3, 10082, 2727, 1086, 4281, 1004, 311, 7536, 173, 1000, 4945, 12, 308, 25786, 12, 28583, 9, 253, 2021, 8, 3946, 63, 6568, 274, 312, 294, 31585, 2535, 1208, 2249, 517, 256, 4281, 2806, 958], [232, 283, 63, 708, 233, 252, 63, 9952, 823, 3243, 31673, 382, 22128, 9, 823, 554, 3577, 179, 294, 253, 2565, 312, 294, 8092, 6015, 13, 1685, 1336, 292, 31146, 256, 1133, 9262, 232, 2111, 63, 1250, 233, 2111, 14, 1556, 1239, 1250, 8, 25786, 12, 28583, 12, 252, 63, 3475, 29, 17, 12, 27022, 63, 9838, 567, 8994, 3807, 340, 2935, 2648, 63, 876, 63, 6603, 567, 2952, 397, 312, 294, 6331, 13400, 497, 4911, 433, 12, 9885, 585, 2535, 1208, 232, 13400, 233, 26497, 83, 8, 1311, 63, 1250, 12, 3096, 12, 1083, 63, 293, 12, 26395, 415, 283, 63, 708, 12, 20785, 382, 283, 63, 708, 12, 370, 8412, 29, 722, 12, 11095, 29, 325, 12, 31447, 29, 678, 9, 232, 13400, 14, 4252, 63], [5657, 323, 232, 522, 233, 821, 14, 2770, 63, 2972, 8, 11193, 14, 3481, 1853, 554, 535, 312, 294, 26674, 7548, 12, 243, 398, 350, 33450, 1187, 626, 232, 296, 226, 12, 283, 63, 513, 253, 2021, 8, 18940, 63, 87, 63, 3467, 274, 298, 294, 23106, 256, 1074, 350, 901, 311, 256, 2535, 222, 283, 63, 22976, 233, 283, 63, 513, 415, 283, 63, 708, 823, 554, 14, 222, 283, 63, 21615, 233, 283, 63, 513, 382, 283, 63, 708, 823, 554, 14, 298, 294, 41640, 433, 1202, 626, 13, 2773, 311, 7536, 222, 1247, 233, 13400, 14, 1556, 1239, 9027, 8, 87, 63, 22976, 12, 283, 63, 21615, 625, 322, 63, 441, 323, 298, 294, 6796, 2600, 5410, 1187, 26513, 296, 1086, 4281, 350, 626, 2535], [222, 1784, 63, 5348, 59, 3946, 12, 226, 61, 233, 635, 14, 2257, 8, 6571, 63, 307, 63, 2988, 8, 9411, 29, 16158, 12, 1247, 29, 56, 12, 522, 29, 89, 12, 7089, 16913, 567, 18714, 63, 21311, 340, 7240, 29, 7549, 12, 7089, 252, 63, 3475, 29, 17, 491, 1612, 29, 16, 9, 173, 173, 5824, 5824, 44032, 173, 3, 6959, 626, 13, 6078, 1354, 173, 173, 3, 1743, 1129, 626, 4281, 624, 173, 1204, 63, 2932, 82, 233, 18140, 13175, 50, 8, 1244, 63, 666, 3411, 3946, 1090, 259, 3946, 491, 1784, 63, 5348, 59, 1075, 14, 9237, 12, 14599, 370, 15535, 63, 87, 63, 3467, 12, 16384, 59, 17, 6640, 396, 9, 173, 173, 38038, 233, 635, 14, 2257, 8, 89, 9, 179, 294, 562], [17813, 1764, 292, 7216, 253, 256, 1676, 173, 1204, 63, 2932, 82, 14, 1389, 929, 16, 547, 10809, 29, 38038, 12, 2081, 861, 1932, 13, 17017, 46902, 438, 2829, 485, 241, 6677, 29, 8436, 14, 5248, 14, 5783, 83, 9, 173]]\n",
      "ATTENTION_MASK: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "LENGTH: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "OVERFLOW_TO_SAMPLE_MAPPING: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for key in outputs:\n",
    "    print(f\"{key.upper()}: {outputs[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ee2763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645ac869fdec448e92545b1dfb0f1746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d03a23220544492943011f7867bb89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 557217\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 93164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a helper function to \"map\"\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55de74c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db3b1e52ad64304b7bb870d7452ceb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 124.24M parameters\n"
     ]
    }
   ],
   "source": [
    "# Model config\n",
    "from transformers import AutoConfig, AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    c_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e70e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 does not have a pad token by default\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f8c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_IDS: torch.Size([4, 128])\n",
      "ATTENTION_MASK: torch.Size([4, 128])\n",
      "LABELS: torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test it out\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(4)])\n",
    "for key in out:\n",
    "    print(f\"{key.upper()}: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5907023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../data/models/codeparrot-ds\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeca294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/workspace/projects/hf_llm/notebooks/wandb/offline-run-20251219_000235-ge603j1e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='690' max='2177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 690/2177 11:48 < 25:31, 0.97 it/s, Epoch 0.32/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c03efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hf credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcea4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"../data/models/codeparrot-ds\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "plt.scatter(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
