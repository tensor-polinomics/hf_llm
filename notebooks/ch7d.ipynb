{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e04d3f",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5255bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ccbf15e4e94c7d91286324875e4bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda1d51ab03843a28a807671f273fde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "multilingual_cc_news.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'maintext', 'url', 'date_publish'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Stream and take only the first 20_000 samples from the Spanish and English datasets\n",
    "spanish_stream = load_dataset(\"intfloat/multilingual_cc_news\", \"es\", split=\"train\", streaming=True).take(20000)\n",
    "english_stream = load_dataset(\"intfloat/multilingual_cc_news\", \"en\", split=\"train\", streaming=True).take(20000)\n",
    "\n",
    "# Convert streams to datasets\n",
    "spanish_dataset = Dataset.from_list(list(spanish_stream))\n",
    "english_dataset = Dataset.from_list(list(english_stream))\n",
    "\n",
    "english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b07e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DatasetDict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_dataset(dataset, train_size=15_000, val_size=2_500, test_size=2_500):\n",
    "    train_dataset = dataset.train_test_split(\n",
    "        train_size=train_size, \n",
    "        seed=42)\n",
    "    val_dataset = train_dataset['test'].select(range(val_size))\n",
    "    test_dataset = train_dataset['test'].select(range(val_size, val_size + test_size))\n",
    "    return DatasetDict({\n",
    "        'train': train_dataset['train'],\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    \n",
    "spanish_dataset = split_dataset(spanish_dataset)\n",
    "english_dataset = split_dataset(english_dataset)\n",
    "english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ddff5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Victim of attempted abduction near RAF Marham to relive ordeal for Crimewatch\n",
      "Maintext (first 200 words): A serviceman at the centre of an attempted abduction near RAF Marham is to relive his ordeal on tomorrow night’s BBC Crimewatch programme.\n",
      "The victim, a married airman in his late 20s, has revealed he\n",
      "\n",
      "Title: Saipan delegation OKs San Roque rezoning\n",
      "Maintext (first 200 words): AFTER a lengthy discussion, the Saipan and Northern Islands Legislative Delegation on Tuesday passed a local measure amending the Saipan Zoning Law to rezone parts of San Roque.\n",
      "Fifteen members voted \n",
      "\n",
      "Title: An unusual new late-night competitor for ESPN\n",
      "Maintext (first 200 words): NEW YORK (AP) — To a certain segment of the population, Scott Van Pelt is a more popular late-night television star than Jimmy Fallon, Jimmy Kimmel and Stephen Colbert.\n",
      "That segment — young men aged 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a few samples\n",
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset['train'].shuffle(seed=seed).select(range(num_samples))\n",
    "    for i, item in enumerate(sample):\n",
    "        print(f\"Title: {item['title']}\")\n",
    "        print(f\"Maintext (first 200 words): {item['maintext'][:200]}\\n\")\n",
    "show_samples(english_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82deafce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Declararán el 21 de octubre como Día de Héctor Espino\n",
      "Maintext (first 200 words): HERMOSILLO, Sonora(GH)\n",
      "En la sesiÃ³n ordinaria correspondiente a septiembre, el Cabildo aprobÃ³ por unanimidad la propuesta de declarar el 21 de octubre como el DÃ­a de HÃ©ctor Espino, en honor al leg\n",
      "\n",
      "Title: Tropical Storm Lisa forms over eastern tropical Atlantic\n",
      "Maintext (first 200 words): \n",
      "\n",
      "Title: Earthquake felt from Nebraska to Texas\n",
      "Maintext (first 200 words): PAWNEE, Okla. (AP) — One of Oklahoma’s largest earthquakes on record rattled other parts of the Midwest on Saturday from Nebraska to North Texas, and likely will turn new attention to the practice of \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the above two datasets (and rename it to be the same as in the LLM course)\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "books_dataset = DatasetDict()\n",
    "\n",
    "for split in english_dataset.keys():\n",
    "    books_dataset[split] = concatenate_datasets(\n",
    "        [english_dataset[split], spanish_dataset[split]])\n",
    "    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
    "    \n",
    "show_samples(books_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c609134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289c4829d79c42d7925bf13d21c41380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99362e51e9654993b34f825d187050c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2843b436ef4f92a9885f3bd9ea4769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish'],\n",
       "        num_rows: 28241\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish'],\n",
       "        num_rows: 4701\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish'],\n",
       "        num_rows: 4699\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out samples with empty title or maintext\n",
    "def filter_empty_samples(dataset):\n",
    "    return dataset.filter(lambda x: len(x['title'].strip()) > 0 and len(x['maintext'].strip()) > 0)\n",
    "books_dataset = DatasetDict({\n",
    "    split: filter_empty_samples(books_dataset[split]) for split in books_dataset.keys()\n",
    "})\n",
    "books_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c85d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Shimon Peres witnessed Israel’s history, and shaped it\n",
      "Maintext (first 200 words): JERUSALEM (AP) — At every corner of Israel’s tumultuous history, Shimon Peres was there.\n",
      "He was a young aide to the nation’s founding fathers when the country declared independence in 1948, and he pla\n",
      "\n",
      "Title: Police: Boy, 2, dies after accidentally shooting self in chest\n",
      "Maintext (first 200 words): QUAKERTOWN, Pa. — State police say a 2-year-old boy accidentally shot himself to death in a Pennsylvania home.\n",
      "Authorities say the boy was pronounced dead Monday shortly after suffering a single gunsh\n",
      "\n",
      "Title: Chiefs' Jamaal Charles doubtful for Week 2 at Houston\n",
      "Maintext (first 200 words): Chiefs running back Jamaal Charles is doubtful for Sunday's game at Houston as he continues to work his way back from surgery last season to repair the torn ACL in his right knee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_samples(books_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c0d5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cf413129fc4143bc462ce38e0f089f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc6be38c7b5446bb6478c4f146629f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138f65dce334428ab511d32707484480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065d6c54fbe74aed8b8110e77985fc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/workspace/projects/hf_llm/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load mt5-small model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2888a9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [336, 3869, 11807, 287, 62893, 295, 12507, 309, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"I love reading the Hunger Games!\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69455827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I', '▁love', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '!', '</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07f5010d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e6e8c8892d439ab1ea9bd948c94237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc0135d241d4008a41b020816c5ff68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c8cabff79e4d6d909a4a672a05e8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing the datasets\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"maintext\"], max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(\n",
    "        examples[\"title\"], max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_books = books_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9829b720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28241\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4701\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'maintext', 'url', 'date_publish', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4699\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40807f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ec2eb933c64e5a9d4faaae66d721c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e70d79eccd4dde96862c0c34f739e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7337f2030e40f1af5970645e7dc7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = books_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1583989c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e153bd3841744869a12ca85e5c874505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.923076923076923),\n",
       " 'rouge2': np.float64(0.7272727272727272),\n",
       " 'rougeL': np.float64(0.923076923076923),\n",
       " 'rougeLsum': np.float64(0.923076923076923)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rouge metric\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "import evaluate\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "results = rouge_score.compute(\n",
    "    predictions=[generated_summary], \n",
    "    references=[reference_summary]\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9c7aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline case using NLTK\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "146a8f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW YORK (AP) — A week before the first presidential debate, Donald Trump is putting moderators on notice that he’ll be watching to see if they get too rough on him. In a series of interviews over the past week, the Republican nominee has asserted that “the system is being rigged” against him. The first of three scheduled debates between Trump and Hillary Clinton will be held on Sept. 26, with NBC’s Lester Holt as the journalist questioning the candidates.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def three_sentence_summary(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return ' '.join(sentences[:3])\n",
    "three_sentence_summary(books_dataset['train'][0]['maintext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16137320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW YORK (AP) — A week before the first presidential debate, Donald Trump is putting moderators on notice that he’ll be watching to see if they get too rough on him.\\nIn a series of interviews over the past week, the Republican nominee has asserted that “the system is being rigged” against him. The first of three scheduled debates between Trump and Hillary Clinton will be held on Sept. 26, with NBC’s Lester Holt as the journalist questioning the candidates.\\n“I think it’s terrible,” Trump told Fox News Channel over the weekend. “They want the host to go after Trump.”\\nHis statement is based on criticism NBC’s Matt Lauer received in some circles for being too easy on the Republican in a forum on national security earlier this month. Trump, who called Lauer “very professional,” told CNBC that he believes this puts pressure on other moderators to avoid Lauer’s fate by going after him.\\nSports fans know the phenomenon as “working the refs.”\\n“Trump’s buddy, the old basketball coach Bobby Knight, used to do this all the time,” said CBS News veteran Bob Schieffer, who moderated a 2012 debate between Barack Obama and Mitt Romney. “He’d throw fits at the referee in the first (10 minutes) and try to make them feel guilty so they’ll give him a break in the (last 10 minutes). That’s all that this is.”\\nIn fact, Trump has twice referenced Knight in recent comments — saying it was his opponents using the former Indiana coach’s tactics.\\nSchieffer’s advice to this year’s moderators is to “laugh it off.” He believes they are skilled and experienced enough to do that.\\n“Every moderator is going to get hammered by somebody,” he said. “That’s just life in the National Football League. This is a big-time deal.”\\nTrump’s tactics could backfire with the public, said Alan Schroeder, author of “Presidential Debates: 50 Years of High-Risk TV.”\\n“To me, it feels like whining,” said Schroeder, a journalism professor at Northeastern University. “These people are running to be president of the United States. They have to deal with a lot of pressure and they have to deal with a lot of circumstances beyond their control … It doesn’t seem very presidential.”\\nModerators should avoid reading and participating in stories about the debates, Schroeder said. He believes they should go further and step away from day-to-day coverage of the campaign, which all of the moderators are involved in to some extent. After Holt, there’s a town hall-style debate moderated by CNN’s Anderson Cooper and ABC’s Martha Raddatz, and a final debate led by Chris Wallace of Fox News.\\nTrump’s opponents have applied pressure, too. David Brock, a Clinton ally and founder of the Media Matters watchdog group, called on the presidential debate commission to drop Wallace because his former boss at Fox, Roger Ailes, is said to be advising Trump. The commission rejected Brock’s request.\\nTrump last week had singled Cooper out for criticism, in an interview with The Washington Post. Trump has repeatedly tweeted criticism of CNN in general over the last few months.\\n“He’ll be very biased, very biased,” Trump said. “I don’t think he should be a moderator. I’ll participate, but I don’t think he should be a moderator. CNN is the Clinton News Network, and Anderson Cooper, I don’t think he can be fair.”\\nCooper and CNN declined comment.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_dataset['train'][0]['maintext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e9d6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to evaluate the baseline\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset['maintext']]\n",
    "    return metric.compute(\n",
    "        predictions=summaries, \n",
    "        references=dataset['title']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab542a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(13.23),\n",
       " 'rouge2': np.float64(5.86),\n",
       " 'rougeL': np.float64(11.23),\n",
       " 'rougeLsum': np.float64(11.32)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the baseline on the validation set\n",
    "import pandas as pd\n",
    "\n",
    "score = evaluate_baseline(books_dataset['validation'], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)\n",
    "rouge_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fb283",
   "metadata": {},
   "source": [
    "### Fine-tuning mT5 with the trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abaf9669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bd47dbf20a455390948c162fb021e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a049a38ff8e403c92bfe0a9cf70ac96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699cd9768e2544c484eed89c1e6cb52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86eb4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Directory settings\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"../data/cache\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # if not using wandb\n",
    "\n",
    "# Traing arguments\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "batch_size = 16\n",
    "num_train_epochs = 3\n",
    "logging_steps = len(tokenized_books['train']) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../data/models/{model_name}-finetuned-amazon-en-es\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd9633d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b70d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Handle tuple output\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # Convert to numpy if tensor\n",
    "    if hasattr(predictions, 'cpu'):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if hasattr(labels, 'cpu'):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    # If 3D logits, get argmax\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Replace invalid token IDs (-100 and out-of-vocab) with pad token\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    predictions = np.where(\n",
    "        (predictions >= 0) & (predictions < vocab_size),\n",
    "        predictions,\n",
    "        tokenizer.pad_token_id\n",
    "    )\n",
    "    predictions = predictions.astype(np.int64)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = labels.astype(np.int64)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred)) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label)) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    \n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c46e11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the collator\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, \n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f07a55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    books_dataset['train'].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a806e446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28241\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4701\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4699\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee49b729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [18737,\n",
       "   852,\n",
       "   123810,\n",
       "   274,\n",
       "   10740,\n",
       "   271,\n",
       "   661,\n",
       "   298,\n",
       "   5219,\n",
       "   5038,\n",
       "   287,\n",
       "   2262,\n",
       "   421,\n",
       "   54532,\n",
       "   259,\n",
       "   45007,\n",
       "   261,\n",
       "   20443,\n",
       "   4396,\n",
       "   339,\n",
       "   421,\n",
       "   37251,\n",
       "   132341,\n",
       "   263,\n",
       "   351,\n",
       "   259,\n",
       "   21280,\n",
       "   533,\n",
       "   790,\n",
       "   293,\n",
       "   1578,\n",
       "   390,\n",
       "   259,\n",
       "   55634,\n",
       "   288,\n",
       "   2354,\n",
       "   955,\n",
       "   287,\n",
       "   276,\n",
       "   1689,\n",
       "   6320,\n",
       "   259,\n",
       "   55904,\n",
       "   351,\n",
       "   4065,\n",
       "   260,\n",
       "   563,\n",
       "   259,\n",
       "   262,\n",
       "   12614,\n",
       "   304,\n",
       "   259,\n",
       "   154144,\n",
       "   910,\n",
       "   287,\n",
       "   9002,\n",
       "   5219,\n",
       "   261,\n",
       "   287,\n",
       "   259,\n",
       "   85803,\n",
       "   259,\n",
       "   142165,\n",
       "   265,\n",
       "   1070,\n",
       "   259,\n",
       "   152986,\n",
       "   345,\n",
       "   533,\n",
       "   359,\n",
       "   1759,\n",
       "   2974,\n",
       "   339,\n",
       "   259,\n",
       "   5330,\n",
       "   1418,\n",
       "   31645,\n",
       "   365,\n",
       "   259,\n",
       "   9825,\n",
       "   4065,\n",
       "   260,\n",
       "   486,\n",
       "   2262,\n",
       "   304,\n",
       "   7156,\n",
       "   31499,\n",
       "   285,\n",
       "   33224,\n",
       "   299,\n",
       "   259,\n",
       "   4964,\n",
       "   4396,\n",
       "   305,\n",
       "   447,\n",
       "   111266,\n",
       "   56550,\n",
       "   898,\n",
       "   390,\n",
       "   14461,\n",
       "   351,\n",
       "   53583,\n",
       "   260,\n",
       "   8156,\n",
       "   514,\n",
       "   259,\n",
       "   85702,\n",
       "   293,\n",
       "   263,\n",
       "   764,\n",
       "   2828,\n",
       "   102435,\n",
       "   527,\n",
       "   287,\n",
       "   52209,\n",
       "   7680,\n",
       "   347,\n",
       "   287,\n",
       "   11079,\n",
       "   299,\n",
       "   260,\n",
       "   359,\n",
       "   566,\n",
       "   5231,\n",
       "   609,\n",
       "   293,\n",
       "   263,\n",
       "   400,\n",
       "   55976,\n",
       "   5224,\n",
       "   4396,\n",
       "   259,\n",
       "   16694,\n",
       "   25183,\n",
       "   1852,\n",
       "   30449,\n",
       "   910,\n",
       "   287,\n",
       "   15699,\n",
       "   260,\n",
       "   359,\n",
       "   10837,\n",
       "   3007,\n",
       "   287,\n",
       "   11285,\n",
       "   288,\n",
       "   1002,\n",
       "   3354,\n",
       "   4396,\n",
       "   1562,\n",
       "   13889,\n",
       "   28176,\n",
       "   339,\n",
       "   259,\n",
       "   5621,\n",
       "   351,\n",
       "   34709,\n",
       "   6410,\n",
       "   259,\n",
       "   85702,\n",
       "   293,\n",
       "   263,\n",
       "   24734,\n",
       "   48185,\n",
       "   295,\n",
       "   11243,\n",
       "   285,\n",
       "   281,\n",
       "   2155,\n",
       "   259,\n",
       "   36638,\n",
       "   263,\n",
       "   332,\n",
       "   259,\n",
       "   5330,\n",
       "   6320,\n",
       "   8778,\n",
       "   351,\n",
       "   287,\n",
       "   259,\n",
       "   85803,\n",
       "   281,\n",
       "   259,\n",
       "   262,\n",
       "   7974,\n",
       "   351,\n",
       "   10811,\n",
       "   20317,\n",
       "   259,\n",
       "   45139,\n",
       "   295,\n",
       "   714,\n",
       "   11400,\n",
       "   260,\n",
       "   4396,\n",
       "   261,\n",
       "   1866,\n",
       "   259,\n",
       "   13075,\n",
       "   48185,\n",
       "   295,\n",
       "   359,\n",
       "   2364,\n",
       "   9588,\n",
       "   5224,\n",
       "   259,\n",
       "   16694,\n",
       "   371,\n",
       "   85702,\n",
       "   533,\n",
       "   790,\n",
       "   259,\n",
       "   14037,\n",
       "   263,\n",
       "   714,\n",
       "   5193,\n",
       "   263,\n",
       "   29107,\n",
       "   351,\n",
       "   1904,\n",
       "   132341,\n",
       "   263,\n",
       "   288,\n",
       "   259,\n",
       "   35897,\n",
       "   48185,\n",
       "   295,\n",
       "   293,\n",
       "   263,\n",
       "   259,\n",
       "   87468,\n",
       "   455,\n",
       "   259,\n",
       "   5846,\n",
       "   3354,\n",
       "   4065,\n",
       "   260,\n",
       "   11340,\n",
       "   24319,\n",
       "   3076,\n",
       "   287,\n",
       "   690,\n",
       "   178535,\n",
       "   272,\n",
       "   527,\n",
       "   359,\n",
       "   81581,\n",
       "   287,\n",
       "   37071,\n",
       "   263,\n",
       "   1562,\n",
       "   359,\n",
       "   150693,\n",
       "   293,\n",
       "   263,\n",
       "   259,\n",
       "   154618,\n",
       "   261,\n",
       "   287,\n",
       "   6801,\n",
       "   259,\n",
       "   87359,\n",
       "   31222,\n",
       "   17408,\n",
       "   1027,\n",
       "   65831,\n",
       "   261,\n",
       "   4165,\n",
       "   288,\n",
       "   342,\n",
       "   714,\n",
       "   751,\n",
       "   287,\n",
       "   1459,\n",
       "   5224,\n",
       "   2426,\n",
       "   259,\n",
       "   76785,\n",
       "   1852,\n",
       "   60545,\n",
       "   17408,\n",
       "   50126,\n",
       "   115141,\n",
       "   261,\n",
       "   1866,\n",
       "   68521,\n",
       "   345,\n",
       "   259,\n",
       "   262,\n",
       "   846,\n",
       "   259,\n",
       "   45007,\n",
       "   259,\n",
       "   4964,\n",
       "   2215,\n",
       "   58149,\n",
       "   20820,\n",
       "   305,\n",
       "   61566,\n",
       "   9470,\n",
       "   9795,\n",
       "   260,\n",
       "   359,\n",
       "   13344,\n",
       "   293,\n",
       "   285,\n",
       "   259,\n",
       "   33195,\n",
       "   7075,\n",
       "   263,\n",
       "   344,\n",
       "   287,\n",
       "   129756,\n",
       "   265,\n",
       "   281,\n",
       "   287,\n",
       "   2262,\n",
       "   13260,\n",
       "   7935,\n",
       "   271,\n",
       "   305,\n",
       "   6557,\n",
       "   288,\n",
       "   2149,\n",
       "   2486,\n",
       "   7671,\n",
       "   81298,\n",
       "   1421,\n",
       "   510,\n",
       "   287,\n",
       "   276,\n",
       "   293,\n",
       "   1578,\n",
       "   4550,\n",
       "   4065,\n",
       "   259,\n",
       "   262,\n",
       "   14744,\n",
       "   281,\n",
       "   287,\n",
       "   274,\n",
       "   6377,\n",
       "   475,\n",
       "   7935,\n",
       "   483,\n",
       "   7961,\n",
       "   293,\n",
       "   263,\n",
       "   751,\n",
       "   533,\n",
       "   714,\n",
       "   339,\n",
       "   1562,\n",
       "   563,\n",
       "   12558,\n",
       "   261,\n",
       "   4396,\n",
       "   1070,\n",
       "   259,\n",
       "   270,\n",
       "   42358,\n",
       "   33749,\n",
       "   285,\n",
       "   65831,\n",
       "   281,\n",
       "   5376,\n",
       "   6483,\n",
       "   661,\n",
       "   3385,\n",
       "   347,\n",
       "   609,\n",
       "   639,\n",
       "   1638,\n",
       "   585,\n",
       "   167445,\n",
       "   259,\n",
       "   4342,\n",
       "   287,\n",
       "   259,\n",
       "   18004,\n",
       "   60438,\n",
       "   31222,\n",
       "   293,\n",
       "   263,\n",
       "   259,\n",
       "   138398,\n",
       "   263,\n",
       "   260,\n",
       "   50126,\n",
       "   115141,\n",
       "   293,\n",
       "   263,\n",
       "   259,\n",
       "   38328,\n",
       "   288,\n",
       "   714,\n",
       "   3721,\n",
       "   293,\n",
       "   263,\n",
       "   132341,\n",
       "   263,\n",
       "   339,\n",
       "   288,\n",
       "   359,\n",
       "   72404,\n",
       "   609,\n",
       "   4368,\n",
       "   1562,\n",
       "   1669,\n",
       "   259,\n",
       "   14037,\n",
       "   263,\n",
       "   287,\n",
       "   276,\n",
       "   418,\n",
       "   32607,\n",
       "   345,\n",
       "   305,\n",
       "   4842,\n",
       "   285,\n",
       "   259,\n",
       "   14821,\n",
       "   288,\n",
       "   342,\n",
       "   533,\n",
       "   260,\n",
       "   359,\n",
       "   478,\n",
       "   2364,\n",
       "   132341,\n",
       "   339,\n",
       "   259,\n",
       "   5846,\n",
       "   288,\n",
       "   1689,\n",
       "   259,\n",
       "   51772,\n",
       "   345,\n",
       "   455,\n",
       "   2155,\n",
       "   4976,\n",
       "   5224,\n",
       "   790,\n",
       "   2426,\n",
       "   260,\n",
       "   359,\n",
       "   95841,\n",
       "   293,\n",
       "   263,\n",
       "   1627,\n",
       "   4256,\n",
       "   281,\n",
       "   287,\n",
       "   5139,\n",
       "   29526,\n",
       "   10667,\n",
       "   260,\n",
       "   1494,\n",
       "   339,\n",
       "   259,\n",
       "   262,\n",
       "   5133,\n",
       "   264,\n",
       "   3315,\n",
       "   19869,\n",
       "   1562,\n",
       "   4396,\n",
       "   293,\n",
       "   263,\n",
       "   259,\n",
       "   138398,\n",
       "   263,\n",
       "   259,\n",
       "   3659,\n",
       "   3004,\n",
       "   32808,\n",
       "   514,\n",
       "   287,\n",
       "   2821,\n",
       "   261,\n",
       "   2426,\n",
       "   31640,\n",
       "   14283,\n",
       "   218055,\n",
       "   261,\n",
       "   10945,\n",
       "   304,\n",
       "   359,\n",
       "   559,\n",
       "   54532,\n",
       "   83756,\n",
       "   299,\n",
       "   267,\n",
       "   1013,\n",
       "   58215,\n",
       "   304,\n",
       "   4577,\n",
       "   264,\n",
       "   582,\n",
       "   5354,\n",
       "   2208,\n",
       "   1562,\n",
       "   359,\n",
       "   4748,\n",
       "   416,\n",
       "   261,\n",
       "   609,\n",
       "   7671,\n",
       "   263,\n",
       "   1469,\n",
       "   87023,\n",
       "   1],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'labels': [188018,\n",
       "   266,\n",
       "   176987,\n",
       "   259,\n",
       "   45007,\n",
       "   261,\n",
       "   4396,\n",
       "   259,\n",
       "   6661,\n",
       "   790,\n",
       "   5231,\n",
       "   263,\n",
       "   2974,\n",
       "   1418,\n",
       "   31645,\n",
       "   1]},\n",
       " {'input_ids': [41236,\n",
       "   265,\n",
       "   259,\n",
       "   57830,\n",
       "   287,\n",
       "   19425,\n",
       "   264,\n",
       "   78219,\n",
       "   11989,\n",
       "   514,\n",
       "   7825,\n",
       "   57256,\n",
       "   11592,\n",
       "   261,\n",
       "   259,\n",
       "   48813,\n",
       "   304,\n",
       "   664,\n",
       "   260,\n",
       "   6597,\n",
       "   52923,\n",
       "   514,\n",
       "   6469,\n",
       "   40541,\n",
       "   260,\n",
       "   48391,\n",
       "   96584,\n",
       "   18878,\n",
       "   259,\n",
       "   57653,\n",
       "   41236,\n",
       "   265,\n",
       "   305,\n",
       "   287,\n",
       "   13858,\n",
       "   1200,\n",
       "   95521,\n",
       "   9048,\n",
       "   287,\n",
       "   13767,\n",
       "   158555,\n",
       "   265,\n",
       "   276,\n",
       "   10291,\n",
       "   344,\n",
       "   287,\n",
       "   654,\n",
       "   5596,\n",
       "   51493,\n",
       "   351,\n",
       "   1716,\n",
       "   27118,\n",
       "   293,\n",
       "   263,\n",
       "   46174,\n",
       "   5596,\n",
       "   3167,\n",
       "   15699,\n",
       "   305,\n",
       "   1607,\n",
       "   9048,\n",
       "   714,\n",
       "   3721,\n",
       "   293,\n",
       "   263,\n",
       "   447,\n",
       "   2810,\n",
       "   259,\n",
       "   67595,\n",
       "   5139,\n",
       "   47915,\n",
       "   260,\n",
       "   486,\n",
       "   259,\n",
       "   147758,\n",
       "   639,\n",
       "   20121,\n",
       "   345,\n",
       "   455,\n",
       "   2725,\n",
       "   11592,\n",
       "   259,\n",
       "   87783,\n",
       "   3026,\n",
       "   2983,\n",
       "   287,\n",
       "   11989,\n",
       "   260,\n",
       "   41236,\n",
       "   265,\n",
       "   259,\n",
       "   57830,\n",
       "   287,\n",
       "   19425,\n",
       "   264,\n",
       "   78219,\n",
       "   11989,\n",
       "   514,\n",
       "   7825,\n",
       "   57256,\n",
       "   11592,\n",
       "   261,\n",
       "   259,\n",
       "   48813,\n",
       "   304,\n",
       "   664,\n",
       "   260,\n",
       "   6597,\n",
       "   52923,\n",
       "   514,\n",
       "   6469,\n",
       "   40541,\n",
       "   260,\n",
       "   16459,\n",
       "   293,\n",
       "   263,\n",
       "   13019,\n",
       "   10893,\n",
       "   345,\n",
       "   259,\n",
       "   123793,\n",
       "   305,\n",
       "   9192,\n",
       "   50697,\n",
       "   263,\n",
       "   344,\n",
       "   287,\n",
       "   41026,\n",
       "   10291,\n",
       "   281,\n",
       "   5766,\n",
       "   264,\n",
       "   101661,\n",
       "   261,\n",
       "   13634,\n",
       "   51493,\n",
       "   281,\n",
       "   67226,\n",
       "   305,\n",
       "   287,\n",
       "   6284,\n",
       "   10291,\n",
       "   281,\n",
       "   82099,\n",
       "   260,\n",
       "   41236,\n",
       "   265,\n",
       "   639,\n",
       "   3302,\n",
       "   344,\n",
       "   13634,\n",
       "   51493,\n",
       "   260,\n",
       "   359,\n",
       "   11780,\n",
       "   293,\n",
       "   263,\n",
       "   259,\n",
       "   262,\n",
       "   3005,\n",
       "   191397,\n",
       "   214490,\n",
       "   5224,\n",
       "   2426,\n",
       "   41236,\n",
       "   265,\n",
       "   260,\n",
       "   359,\n",
       "   11780,\n",
       "   293,\n",
       "   263,\n",
       "   2101,\n",
       "   259,\n",
       "   262,\n",
       "   3005,\n",
       "   3721,\n",
       "   304,\n",
       "   259,\n",
       "   59727,\n",
       "   527,\n",
       "   259,\n",
       "   262,\n",
       "   11912,\n",
       "   304,\n",
       "   287,\n",
       "   1200,\n",
       "   95521,\n",
       "   4644,\n",
       "   1562,\n",
       "   102096,\n",
       "   792,\n",
       "   486,\n",
       "   67226,\n",
       "   366,\n",
       "   157850,\n",
       "   263,\n",
       "   23853,\n",
       "   285,\n",
       "   12693,\n",
       "   634,\n",
       "   30334,\n",
       "   21734,\n",
       "   172641,\n",
       "   288,\n",
       "   287,\n",
       "   764,\n",
       "   807,\n",
       "   39923,\n",
       "   259,\n",
       "   131261,\n",
       "   263,\n",
       "   332,\n",
       "   259,\n",
       "   262,\n",
       "   23852,\n",
       "   473,\n",
       "   17393,\n",
       "   281,\n",
       "   287,\n",
       "   598,\n",
       "   6539,\n",
       "   4171,\n",
       "   259,\n",
       "   124599,\n",
       "   260,\n",
       "   359,\n",
       "   142831,\n",
       "   1425,\n",
       "   259,\n",
       "   262,\n",
       "   2316,\n",
       "   13861,\n",
       "   714,\n",
       "   3721,\n",
       "   1156,\n",
       "   514,\n",
       "   287,\n",
       "   5233,\n",
       "   304,\n",
       "   14644,\n",
       "   15597,\n",
       "   52478,\n",
       "   305,\n",
       "   287,\n",
       "   18072,\n",
       "   304,\n",
       "   531,\n",
       "   63453,\n",
       "   926,\n",
       "   807,\n",
       "   787,\n",
       "   259,\n",
       "   23145,\n",
       "   288,\n",
       "   4550,\n",
       "   4065,\n",
       "   259,\n",
       "   262,\n",
       "   12862,\n",
       "   288,\n",
       "   5233,\n",
       "   281,\n",
       "   287,\n",
       "   259,\n",
       "   41245,\n",
       "   5224,\n",
       "   2426,\n",
       "   30086,\n",
       "   82211,\n",
       "   9917,\n",
       "   8311,\n",
       "   260,\n",
       "   21734,\n",
       "   172641,\n",
       "   259,\n",
       "   15484,\n",
       "   345,\n",
       "   281,\n",
       "   7156,\n",
       "   786,\n",
       "   79313,\n",
       "   10239,\n",
       "   714,\n",
       "   11989,\n",
       "   305,\n",
       "   1425,\n",
       "   44252,\n",
       "   264,\n",
       "   68368,\n",
       "   8449,\n",
       "   514,\n",
       "   259,\n",
       "   262,\n",
       "   259,\n",
       "   112276,\n",
       "   15967,\n",
       "   259,\n",
       "   33733,\n",
       "   305,\n",
       "   259,\n",
       "   262,\n",
       "   17042,\n",
       "   353,\n",
       "   259,\n",
       "   49968,\n",
       "   259,\n",
       "   9825,\n",
       "   259,\n",
       "   20935,\n",
       "   260,\n",
       "   486,\n",
       "   366,\n",
       "   264,\n",
       "   59525,\n",
       "   5169,\n",
       "   287,\n",
       "   11989,\n",
       "   281,\n",
       "   66371,\n",
       "   351,\n",
       "   18132,\n",
       "   260,\n",
       "   10136,\n",
       "   792,\n",
       "   486,\n",
       "   11419,\n",
       "   6765,\n",
       "   5407,\n",
       "   293,\n",
       "   263,\n",
       "   18195,\n",
       "   4644,\n",
       "   68405,\n",
       "   332,\n",
       "   3302,\n",
       "   344,\n",
       "   287,\n",
       "   564,\n",
       "   26008,\n",
       "   371,\n",
       "   88915,\n",
       "   10291,\n",
       "   514,\n",
       "   259,\n",
       "   262,\n",
       "   2725,\n",
       "   18575,\n",
       "   304,\n",
       "   259,\n",
       "   51020,\n",
       "   11964,\n",
       "   4288,\n",
       "   344,\n",
       "   65767,\n",
       "   4908,\n",
       "   60051,\n",
       "   10136,\n",
       "   4721,\n",
       "   351,\n",
       "   19234,\n",
       "   261,\n",
       "   281,\n",
       "   259,\n",
       "   262,\n",
       "   21617,\n",
       "   304,\n",
       "   475,\n",
       "   259,\n",
       "   64701,\n",
       "   260,\n",
       "   486,\n",
       "   371,\n",
       "   88915,\n",
       "   263,\n",
       "   259,\n",
       "   47149,\n",
       "   9171,\n",
       "   259,\n",
       "   48618,\n",
       "   263,\n",
       "   259,\n",
       "   25386,\n",
       "   2262,\n",
       "   264,\n",
       "   24657,\n",
       "   27039,\n",
       "   295,\n",
       "   336,\n",
       "   141362,\n",
       "   260,\n",
       "   23088,\n",
       "   263,\n",
       "   3690,\n",
       "   7648,\n",
       "   12699,\n",
       "   305,\n",
       "   15985,\n",
       "   265,\n",
       "   56934,\n",
       "   67005,\n",
       "   259,\n",
       "   47149,\n",
       "   68405,\n",
       "   332,\n",
       "   9171,\n",
       "   807,\n",
       "   344,\n",
       "   10774,\n",
       "   15403,\n",
       "   39893,\n",
       "   260,\n",
       "   1],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'labels': [259,\n",
       "   57653,\n",
       "   41236,\n",
       "   265,\n",
       "   261,\n",
       "   13858,\n",
       "   1200,\n",
       "   95521,\n",
       "   13019,\n",
       "   447,\n",
       "   2810,\n",
       "   259,\n",
       "   67595,\n",
       "   43403,\n",
       "   263,\n",
       "   11989,\n",
       "   10331,\n",
       "   1]},\n",
       " {'input_ids': [5258,\n",
       "   898,\n",
       "   390,\n",
       "   375,\n",
       "   22773,\n",
       "   702,\n",
       "   356,\n",
       "   259,\n",
       "   262,\n",
       "   260,\n",
       "   282,\n",
       "   260,\n",
       "   288,\n",
       "   419,\n",
       "   421,\n",
       "   260,\n",
       "   282,\n",
       "   260,\n",
       "   18132,\n",
       "   261,\n",
       "   53583,\n",
       "   260,\n",
       "   8051,\n",
       "   351,\n",
       "   4437,\n",
       "   6452,\n",
       "   281,\n",
       "   259,\n",
       "   89439,\n",
       "   702,\n",
       "   48086,\n",
       "   51540,\n",
       "   288,\n",
       "   39628,\n",
       "   807,\n",
       "   29317,\n",
       "   305,\n",
       "   287,\n",
       "   15068,\n",
       "   259,\n",
       "   1679,\n",
       "   259,\n",
       "   16289,\n",
       "   6466,\n",
       "   304,\n",
       "   4437,\n",
       "   6452,\n",
       "   351,\n",
       "   21115,\n",
       "   261,\n",
       "   259,\n",
       "   77792,\n",
       "   261,\n",
       "   20913,\n",
       "   807,\n",
       "   305,\n",
       "   43480,\n",
       "   807,\n",
       "   259,\n",
       "   125970,\n",
       "   263,\n",
       "   332,\n",
       "   259,\n",
       "   262,\n",
       "   2351,\n",
       "   9534,\n",
       "   435,\n",
       "   368,\n",
       "   305,\n",
       "   2351,\n",
       "   3153,\n",
       "   260,\n",
       "   1],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'labels': [259,\n",
       "   61403,\n",
       "   259,\n",
       "   102731,\n",
       "   263,\n",
       "   332,\n",
       "   2969,\n",
       "   259,\n",
       "   182221,\n",
       "   8675,\n",
       "   281,\n",
       "   2554,\n",
       "   1]}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [tokenized_datasets['train'][i] for i in range(3)]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a544ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4370/2236513724.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b6adc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5298' max='5298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5298/5298 28:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.131900</td>\n",
       "      <td>3.148026</td>\n",
       "      <td>30.744200</td>\n",
       "      <td>15.675900</td>\n",
       "      <td>28.284700</td>\n",
       "      <td>28.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.123700</td>\n",
       "      <td>2.673236</td>\n",
       "      <td>30.892800</td>\n",
       "      <td>15.778800</td>\n",
       "      <td>28.387900</td>\n",
       "      <td>28.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.021100</td>\n",
       "      <td>2.666648</td>\n",
       "      <td>31.182100</td>\n",
       "      <td>15.939200</td>\n",
       "      <td>28.663700</td>\n",
       "      <td>28.626000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5298, training_loss=2.7588231835377894, metrics={'train_runtime': 1689.2697, 'train_samples_per_second': 50.154, 'train_steps_per_second': 3.136, 'total_flos': 4.479714956000256e+16, 'train_loss': 2.7588231835377894, 'epoch': 3.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7aed352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.6666476726531982,\n",
       " 'eval_rouge1': 31.1821,\n",
       " 'eval_rouge2': 15.9392,\n",
       " 'eval_rougeL': 28.6637,\n",
       " 'eval_rougeLsum': 28.626,\n",
       " 'eval_runtime': 94.1273,\n",
       " 'eval_samples_per_second': 49.943,\n",
       " 'eval_steps_per_second': 3.123,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eval\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bb0025d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba37a9ca93454f76a0ffc3029698339c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a2cbb334af4f2092b72156893361d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tensor-polinomics/mt5-small-finetuned-amazon-en-es/commit/2081461e202a131146025890e79c937d2f9a5d35', commit_message='First commit', commit_description='', oid='2081461e202a131146025890e79c937d2f9a5d35', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tensor-polinomics/mt5-small-finetuned-amazon-en-es', endpoint='https://huggingface.co', repo_type='model', repo_id='tensor-polinomics/mt5-small-finetuned-amazon-en-es'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to the Hub\n",
    "# Set up hf credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "\n",
    "trainer.push_to_hub(tags=\"summarization\", commit_message=\"First commit\", token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941cd53f",
   "metadata": {},
   "source": [
    "### Fine-tuning mT5 with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fe86f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5508110fad694967a8bd04da4017d81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/projects/hf_llm/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: {'rouge1': np.float64(12.7479), 'rouge2': np.float64(3.8323), 'rougeL': np.float64(11.6365), 'rougeLsum': np.float64(11.6514)}\n",
      "Epoch 1: {'rouge1': np.float64(14.8244), 'rouge2': np.float64(5.1632), 'rougeL': np.float64(13.5279), 'rougeLsum': np.float64(13.5317)}\n",
      "Epoch 2: {'rouge1': np.float64(15.1101), 'rouge2': np.float64(5.3491), 'rougeL': np.float64(13.8103), 'rougeLsum': np.float64(13.8224)}\n",
      "Repo created: tensor-polinomics/mt5-finetuned-en-es-accelerate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9188a598a504db39b4c9a717ab53328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c459a0e6139407983f862b71d28f2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a555c7c0ffd14cd396c0a0972a824b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3b90ee84cb4163b912f69a5baed35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c93ccddcb74a5cb28cdbe7f28e30d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up data format to tensor\n",
    "tokenized_datasets.set_format(\n",
    "    type='torch', \n",
    ")\n",
    "\n",
    "# Re-load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_checkpoint\n",
    ")\n",
    "\n",
    "# Define dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "from transformers import get_scheduler\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Postprocessing function\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# Training loop\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    result = rouge_score.compute()\n",
    "    result = {key: round(value * 100, 4) for key, value in result.items()}\n",
    "    print(f\"Epoch {epoch}: {result}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save to the Hub\n",
    "model_name = \"mt5-finetuned-en-es-accelerate\"\n",
    "\n",
    "# Create repo\n",
    "repo_id = create_repo(model_name, token=token, exist_ok=True).repo_id\n",
    "print(f\"Repo created: {repo_id}\")\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(\n",
    "    \"../data/models/mt5-finetuned-en-es-accelerate\",\n",
    "    save_function=accelerator.save,\n",
    ")\n",
    "if accelerator.is_main_process:\n",
    "    tokenizer.save_pretrained(\"../data/models/mt5-finetuned-en-es-accelerate\")\n",
    "    unwrapped_model.push_to_hub(model_name, token=token)\n",
    "    tokenizer.push_to_hub(model_name, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4002007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6a15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
