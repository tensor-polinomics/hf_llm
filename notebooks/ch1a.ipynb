{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56784b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b82c79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9989345669746399}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love using transformers library!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0b58b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9980935454368591},\n",
       " {'label': 'NEGATIVE', 'score': 0.9974516034126282}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I have been waiting for a Hugging Face course my whole life.\",\n",
    "     \"Today is really freezing cold in NYC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e2824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445994257926941, 0.11197388917207718, 0.0434267558157444]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "result = classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a62a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education: 0.8446\n",
      "business: 0.1120\n",
      "politics: 0.0434\n"
     ]
    }
   ],
   "source": [
    "for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "    print(f\"{label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceffbfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Output 1 ===\n",
      "In a distant future, humanity has gone through a period of political and economic upheaval, when the people's ability to live in peace and prosperity has been reduced to mere survival. The United States has had to face a series of crises, the first of which was caused by the Soviet Union, which destroyed its position as the world's largest economy, and the second by the U.S. invasion of Iraq.\n",
      "\n",
      "But as time goes on, the United States has changed its mind. And the most important change is the fact that the United States is now in a position to act in its own interests.\n",
      "\n",
      "What is the U.S. doing to make the world a better place?\n",
      "\n",
      "The United States is taking steps to create a new order in which the United States and its allies are able to do things that were unthinkable in the Cold War. The United States is beginning to develop a much more sophisticated military and intelligence apparatus to fight its enemies. The U.S. is also working toward a much more balanced political system. And as recently as the first quarter of this year, President Obama announced that he would seek to reestablish the United States as a member of the Council on Foreign Relations, the body that advises the president and Congress.\n",
      "\n",
      "What is the U.S. doing\n",
      "=== Output 2 ===\n",
      "In a distant future, humanity has been able to use the same technology that will enable the development of nuclear weapons.\n",
      "\n",
      "The next step is to develop a \"new nuclear weapon.\"\n",
      "\n",
      "For more information about nuclear weapons, please visit:\n",
      "\n",
      "http://www.australian.com/news/nuclear-weapons-and-the-new-nuclear-weapon-news-filedin-05\n",
      "\n",
      "http://www.theamericantelegraph.co.uk/news-world/article/1.86957/AUTOCROP/h342/Nuclear-Weapons-And-The-New-Nuclear-Weapon-1715-20130901-i.html\n",
      "\n",
      "© The Author 2014. Published by Oxford University Press on behalf of the World Health Organization. All rights reserved. For Permissions, please email: journals.permissions@oup.com\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "output = generator(\"In a distant future, humanity has\", max_length=50, num_return_sequences=2)\n",
    "for i, out in enumerate(output):\n",
    "    print(f\"=== Output {i+1} ===\")\n",
    "    print(out[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49167db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In a distant future, humanity has gone through a period of political and economic upheaval, when the people's ability to live in peace and prosperity has been reduced to mere survival. The United States has had to face a series of crises, the first of which was caused by the Soviet Union, which destroyed its position as the world's largest economy, and the second by the U.S. invasion of Iraq.\\n\\nBut as time goes on, the United States has changed its mind. And the most important change is the fact that the United States is now in a position to act in its own interests.\\n\\nWhat is the U.S. doing to make the world a better place?\\n\\nThe United States is taking steps to create a new order in which the United States and its allies are able to do things that were unthinkable in the Cold War. The United States is beginning to develop a much more sophisticated military and intelligence apparatus to fight its enemies. The U.S. is also working toward a much more balanced political system. And as recently as the first quarter of this year, President Obama announced that he would seek to reestablish the United States as a member of the Council on Foreign Relations, the body that advises the president and Congress.\\n\\nWhat is the U.S. doing\"},\n",
       " {'generated_text': 'In a distant future, humanity has been able to use the same technology that will enable the development of nuclear weapons.\\n\\nThe next step is to develop a \"new nuclear weapon.\"\\n\\nFor more information about nuclear weapons, please visit:\\n\\nhttp://www.australian.com/news/nuclear-weapons-and-the-new-nuclear-weapon-news-filedin-05\\n\\nhttp://www.theamericantelegraph.co.uk/news-world/article/1.86957/AUTOCROP/h342/Nuclear-Weapons-And-The-New-Nuclear-Weapon-1715-20130901-i.html\\n\\n© The Author 2014. Published by Oxford University Press on behalf of the World Health Organization. All rights reserved. For Permissions, please email: journals.permissions@oup.com'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787c3328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03617250174283981,\n",
       "  'token': 652,\n",
       "  'token_str': ' face',\n",
       "  'sequence': 'Hugging Face is creating a face that the world needs.'},\n",
       " {'score': 0.03370831161737442,\n",
       "  'token': 13327,\n",
       "  'token_str': ' selfie',\n",
       "  'sequence': 'Hugging Face is creating a selfie that the world needs.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"Hugging Face is creating a <mask> that the world needs.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2598d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "/mnt/ebs1/yluo/projects/learning/learning_misc/hf_llm/.venv/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9990031),\n",
       "  'word': 'Nvidia',\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.99806106),\n",
       "  'word': 'Santa Clara',\n",
       "  'start': 68,\n",
       "  'end': 79}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"Nvidia is a dominant player in the global AI race, headquartered in Santa Clara.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a46e276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.48460853099823,\n",
       " 'start': 16,\n",
       " 'end': 36,\n",
       " 'answer': 'a technology company'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"What is Hugging Face?\",\n",
    "    context=\"Hugging Face is a technology company based in New York and Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da54ef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42528e476d7a4298a388f58f77c7097b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2668ec18196c4b6c953c1bbb68664139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c8b96f9cd9484db18810f29e034a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d5e524629b452f9d15f4b85012ea08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea5273a3142420d82fa89f2ec77f7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c008365550ae401eb6581fdf5f263606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' NVIDIA is a global technology company best known for pioneering the modern GPU (graphics processing unit) Founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem, NVIDIA began as a leader in computer graphics . Today, NVIDIA sits at the center of the AI revolution .'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/nvidia_short.txt\", \"r\") as f:\n",
    "    context = f.read()\n",
    "    \n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02698911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b2669b072d43cc878d72079f56c168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd65e93bd204360b5cf3a2491e7f6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19f82d70afc424bb1b6ba38ab940bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813b53e215a04621931e56e3874b0b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96875a70d406488f9f6fc1d110b164ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e794b0861a96429fbc3c933b68dfdd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/806k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f475dc0c8b78467aaf0e2e9f9d00a161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4786b4fab90d4694b9d4577ba2193110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ebs1/yluo/projects/learning/learning_misc/hf_llm/.venv/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '抱抱脸正在创造一个世界需要的工具。'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English to Chinese translation\n",
    "translater = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "translater(\"Hugging Face is creating a tool that the world needs.\", max_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a3e1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'lynx, catamount', 'score': 0.4334997534751892},\n",
       " {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
       "  'score': 0.03479616343975067},\n",
       " {'label': 'snow leopard, ounce, Panthera uncia',\n",
       "  'score': 0.032401930540800095},\n",
       " {'label': 'Egyptian cat', 'score': 0.023944787681102753},\n",
       " {'label': 'tiger cat', 'score': 0.02288924530148506}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_classifier = pipeline(\n",
    "    \"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "image_classifier(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90f75a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install ffmpeg which is required for audio processing\n",
    "\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d353acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up hugging face token and inference client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "load_dotenv()\n",
    "client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7aba57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 384\n",
      "First 5 values: [-0.06439797  0.02905345  0.00817702 -0.0061232   0.01122601]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = client.feature_extraction(\n",
    "    \"This is a test sentence.\",\n",
    "    model=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"Dimension: {embeddings.shape[0]}\")  # 384\n",
    "print(f\"First 5 values: {embeddings[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4996ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8, 384)\n",
      "CPU Time: 0.033s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load model (first time downloads ~90MB)\n",
    "embedder = pipeline(\"feature-extraction\", model=\"BAAI/bge-small-en-v1.5\", device=\"cpu\")\n",
    "\n",
    "# Single sentence\n",
    "sentence = \"This is a test sentence.\"\n",
    "\n",
    "start = time.time()\n",
    "embeddings = embedder(sentence)\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "embeddings = np.array(embeddings).squeeze()\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"CPU Time: {cpu_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56b087ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f15cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
